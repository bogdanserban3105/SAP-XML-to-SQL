{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2ad4d5-51be-4bc8-ba81-aaa294dd61eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql.functions import split, first, collect_list, concat_ws, lit\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session \n",
    "spark = SparkSession.builder.appName(\"Merge Example\").getOrCreate()\n",
    "\n",
    "# # Set your storage account SAS token and name\n",
    "# sa = \"erdccalearning\"  # Replace with your actual storage account name\n",
    "# container_name = \"nndemo\"  # Replace with your actual container name\n",
    "\n",
    "# spark.conf.set(f\"fs.azure.account.auth.type.{sa}.dfs.core.windows.net\", \"SAS\")\n",
    "# spark.conf.set(f\"fs.azure.sas.token.provider.type.{sa}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "# spark.conf.set(f\"fs.azure.sas.nndemo.{sa}.blob.core.windows.net\", f\"{sasToken}\")\n",
    "\n",
    "sa = \"stlpdel01dev\"\n",
    "container_name = \"codeconverter\" # Replace with your actual container name\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"CustomAccessToken\")\n",
    "spark.conf.set(\"fs.azure.account.custom.token.provider.class\", spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\"))\n",
    "\n",
    "# Set the base URL for the container and directories\n",
    "base_path = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/InputFiles/\"\n",
    "\n",
    "# Function to list all directories (DF1, DF2, ..., DF48)\n",
    "def list_subdirectories(base_path):\n",
    "    directories = []\n",
    "    # List files and directories under base_path\n",
    "    for file_info in dbutils.fs.ls(base_path):\n",
    "        if file_info.isDir() and file_info.name.startswith(\"DF\"):  # Check for directories DF1, DF2, ...\n",
    "            directories.append(file_info.path)\n",
    "    return directories\n",
    "\n",
    "# Function to list all files in a given directory (no filtering by extension)\n",
    "def list_all_files(directory_path):\n",
    "    all_files = []\n",
    "    for file_info in dbutils.fs.ls(directory_path):\n",
    "        if file_info.isDir():\n",
    "            # Recursively list files in subdirectories\n",
    "            all_files.extend(list_all_files(file_info.path))\n",
    "        else:\n",
    "            # Add file to the list\n",
    "            all_files.append(file_info.path)\n",
    "    return all_files\n",
    "\n",
    "# Function to extract 'defaultDescription' from XML in the file using Spark DataFrame\n",
    "def extract_description(file_path):\n",
    "    try:\n",
    "        # Read the file content using Spark DataFrame\n",
    "        df = spark.read.text(file_path)\n",
    "        \n",
    "        # Collect all the lines in the file as a list\n",
    "        xml_content = df.rdd.map(lambda row: row[0]).collect()\n",
    "        \n",
    "        # Join the list of lines into a single string representing the XML content\n",
    "        xml_string = \"\\n\".join(xml_content)\n",
    "        \n",
    "        # Parse the XML content using ElementTree\n",
    "        root = ET.fromstring(xml_string)\n",
    "        \n",
    "        # Find the <descriptions> tag and get the 'defaultDescription' attribute\n",
    "        descriptions = root.findall('.//descriptions')  # Find all <descriptions> tags\n",
    "        \n",
    "        for description in descriptions:\n",
    "            # Get the 'defaultDescription' attribute\n",
    "            default_description = description.get('defaultDescription')\n",
    "            return default_description\n",
    "        \n",
    "        return \"Descriptions tag not found\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file: {e}\"\n",
    "\n",
    "# Main logic to process all directories and files\n",
    "def main():\n",
    "    # List to hold file names, subdirectory names, and their corresponding descriptions\n",
    "    file_data = []\n",
    "    \n",
    "    # List all DF directories (DF1, DF2, ..., DF48)\n",
    "    directories = list_subdirectories(base_path)\n",
    "    \n",
    "    # Loop through each directory\n",
    "    for directory in directories:\n",
    "        print(f\"Processing directory: {directory}\")\n",
    "        \n",
    "        # Extract the subdirectory name (e.g., DF1, DF2, etc.)\n",
    "        subdirectory_name = os.path.basename(directory.rstrip('/'))\n",
    "        \n",
    "        # List all files in the directory (no filtering)\n",
    "        all_files = list_all_files(directory)\n",
    "        \n",
    "        # Process each file\n",
    "        for file_path in all_files:\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            # Extract the description for the file\n",
    "            description = extract_description(file_path)\n",
    "            \n",
    "            # Get the file name (last part of the path)\n",
    "            file_name = os.path.basename(file_path)\n",
    "            \n",
    "            # Determine the SourceSystem based on the file name\n",
    "            # If the file name ends with 'calculationview', set SourceSystem to 'HANA'\n",
    "            if file_name.lower().endswith('calculationview'):\n",
    "                source_system = \"HANA\"\n",
    "            # If the file name ends with '.html' (case-insensitive), set SourceSystem to 'BW'\n",
    "            elif file_name.lower().endswith('.html'):\n",
    "                source_system = \"BW\"\n",
    "            else:\n",
    "                source_system = \"Other\"\n",
    "            \n",
    "            # Append the file name, subdirectory name, description, and source system to the list\n",
    "            file_data.append((subdirectory_name, file_name, description, source_system))\n",
    "    \n",
    "    # Create a DataFrame from the file_data list\n",
    "    if file_data:\n",
    "        df = spark.createDataFrame(file_data, [\"DataFlow\", \"FileName\", \"Description\", \"SourceSystem\"])\n",
    "        \n",
    "        # Create the DataBricksDescription by splitting Description (if applicable)\n",
    "        df = df.withColumn(\"DataBricksDescription\", split(df[\"Description\"], \"CV_\")[1])\n",
    "        \n",
    "        # Show the DataFrame\n",
    "        df.show()\n",
    "   \n",
    "    return df\n",
    "\n",
    "# Run the script\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dea4c7f-cb3b-4f74-9d6d-87fa5ee03371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594bb0be-5477-4e64-bf60-2f76c9b882f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming df1 is your DataFrame\n",
    "df1 = df  # Copy your DataFrame\n",
    "\n",
    "# Create a temporary view for the aggregated DataFrame\n",
    "df1.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "# Create or replace the temporary target table from the original table\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW flowtableinfo_temp AS\n",
    "SELECT * FROM codeconverter_config.dataflowtableinfo\n",
    "\"\"\")\n",
    "\n",
    "# Execute the MERGE SQL query on the temporary table\n",
    "merge_query_temp = \"\"\"\n",
    "MERGE INTO flowtableinfo_temp AS tv\n",
    "USING temp_table AS tt\n",
    "ON tv.DataFlow = tt.DataFlow AND tv.SAPFileName = tt.FileName\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET\n",
    "        tv.DataBricksTableName = tt.DataBricksDescription,\n",
    "        tv.SAPTableName = tt.Description,\n",
    "        tv.SourceSystem = tt.SourceSystem  -- Added SourceSystem update\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (DataFlow, SAPFileName, SAPTableName, DataBricksTableName, SourceSystem, group_id)\n",
    "    VALUES (tt.DataFlow, tt.FileName, tt.Description, tt.DataBricksDescription, tt.SourceSystem, NULL);\n",
    "\"\"\"\n",
    "spark.sql(merge_query_temp)\n",
    "\n",
    "# Update the original table with the content of the temporary table\n",
    "update_original_query = \"\"\"\n",
    "MERGE INTO codeconverter_config.dataflowtableinfo AS orig\n",
    "USING flowtableinfo_temp AS temp\n",
    "ON orig.DataFlow = temp.DataFlow AND orig.SAPFileName = temp.SAPFileName\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET\n",
    "        orig.SAPTableName = temp.SAPTableName,\n",
    "        orig.DataBricksTableName = temp.DataBricksTableName,\n",
    "        orig.SchemaName = temp.SchemaName,\n",
    "        orig.SourceSystem = temp.SourceSystem,  -- Added SourceSystem update\n",
    "        orig.group_id = temp.group_id\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (DataFlow, SAPFileName, SAPTableName, DataBricksTableName, SchemaName, SourceSystem, group_id)\n",
    "    VALUES (temp.DataFlow, temp.SAPFileName, temp.SAPTableName, temp.DataBricksTableName, temp.SchemaName, temp.SourceSystem, temp.group_id);\n",
    "\"\"\"\n",
    "spark.sql(update_original_query)\n",
    "\n",
    "# Retrieve and show results from the original target table\n",
    "result_df = spark.sql(\"SELECT * FROM codeconverter_config.dataflowtableinfo\")\n",
    "result_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "extract_filename_description_from_xml_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
