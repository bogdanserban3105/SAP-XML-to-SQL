{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9b01097-15ac-4280-aec8-a7d3f20db364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1195617-d6ba-4c5f-b452-97bebab92901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da33431a-cb27-4586-9347-6b841cc8f8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "is_target_notebook = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5ad04f-94b6-459b-8d86-672c710b32cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "sa = \"stlpdel01dev\"\n",
    "container_name = \"codeconverter\" # Replace with your actual container name\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"CustomAccessToken\")\n",
    "spark.conf.set(\"fs.azure.account.custom.token.provider.class\", spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1db04da-af16-4836-b51c-a7e29efd5318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Get parameters value\n",
    "dbutils.widgets.text(\"data_flow_insertion\", \"DF34\", \"Choose max number of tokens\")\n",
    "data_flow_man = dbutils.widgets.get(\"data_flow_insertion\")\n",
    "data_flow_man = data_flow_man.upper()\n",
    "print(f\"Current working directory: {data_flow_man}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f1f396-b3b4-47d9-9cb1-590ae461269e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get file info from config table (DataFlowTableInfo)\n",
    "get_dataflowbaseinfo = f\"\"\"\n",
    "    SELECT * FROM codeconverter_config.dataflowbaseinfo\n",
    "    WHERE DataFlow = '{data_flow_man}'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and assign to a DataFrame\n",
    "result_flowbaseinfo = spark.sql(get_dataflowbaseinfo)\n",
    "\n",
    "# Get values from DataFlowBaseInfo table\n",
    "dataflow_value = result_flowbaseinfo.select('DataFlow').first()[0]\n",
    "dataflow_files_sourcepath_value = result_flowbaseinfo.select('DataFlowFilesSourceFolder').first()[0]\n",
    "prompt_value = result_flowbaseinfo.select('PathToPrompt').first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de6a735-223e-4714-a1f4-d40755130bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"DataFlow: \" + dataflow_value, \"Source Path: \" + dataflow_files_sourcepath_value, \"Prompt Path: \" + prompt_value, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb386ed-030a-46dc-b6bc-f5c33157e9a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Create a dropdown widget for selecting model type (o1 or o1 mini)\n",
    "dbutils.widgets.dropdown(\"MAX_TOKENS\", \"120000\", [\"150000\", \"120000\", \"100000\"], \"Choose max number of tokens\")\n",
    "# Retrieve the selected model type\n",
    "MAX_TOKENS =int(dbutils.widgets.get(\"MAX_TOKENS\"))\n",
    "print(f\"Selected model: {MAX_TOKENS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e17f3db-24d6-4c89-9461-9bdd2aff62d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the base URL for the container and directories\n",
    "base_url = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/\"\n",
    "\n",
    "# Set the source directory path based on the selected folder\n",
    "source_directory_path = f\"{base_url}{dataflow_files_sourcepath_value}/\"\n",
    "\n",
    "# Create subdirectories under ValidatedFiles, ArchivedFiles, and Logs using the data_flow_sel\n",
    "target_directory_path = f\"{base_url}ValidatedFiles/{data_flow_man}/\"\n",
    "archive_directory_path = f\"{base_url}ArchivedFiles/{data_flow_man}/\"\n",
    "\n",
    "# Create an archive folder with the current date\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "archive_date_folder = f\"{archive_directory_path.rstrip('/')}/{current_date}/\"\n",
    "dbutils.fs.mkdirs(archive_date_folder)\n",
    "\n",
    "# Create a logs directory in the Blob Storage container\n",
    "logs_directory_path = f\"{base_url}Logs/{data_flow_man}/{current_date}/\"\n",
    "dbutils.fs.mkdirs(logs_directory_path)\n",
    "\n",
    "# Define Prompt Path\n",
    "prompt_path = base_url + prompt_value\n",
    "\n",
    "print(base_url)\n",
    "print(source_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd3a8ed-8c3e-4bbe-a76c-c1414592f86c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Prompt Path: \" + prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2a7e5b-c1fd-474c-9c18-2b33f005398a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files in the source directory\n",
    "files = dbutils.fs.ls(source_directory_path)\n",
    "\n",
    "\n",
    "# Retrieve the HANA-related files\n",
    "hana_files_query = f\"SELECT SAPFileName FROM codeconverter_config.DataFlowTableInfo WHERE DataFlow = '{data_flow_man}' AND SourceSystem = 'HANA'\"\n",
    "hana_files_df = spark.sql(hana_files_query)\n",
    "\n",
    "# Convert the result into a list of file names\n",
    "hana_files = [row['SAPFileName'] for row in hana_files_df.collect()]\n",
    "\n",
    "print(hana_files)\n",
    "\n",
    "# Keep only files with source system HANA\n",
    "files = [file for file in files if file.name in hana_files]\n",
    "\n",
    "# Initialize a list to store all log messages\n",
    "log_messages = []\n",
    "\n",
    "# Configuration query\n",
    "get_dataflowtableinfo = f\"\"\"\n",
    "SELECT * FROM codeconverter_config.DataFlowTableInfo\n",
    "WHERE DataFlow = '{data_flow_man}'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and convert the results to Pandas DataFrame\n",
    "result_flowtableinfo = spark.sql(get_dataflowtableinfo)\n",
    "metadata_pd = result_flowtableinfo.toPandas()\n",
    "\n",
    "# Cast 'group_id' column to object dtype and fill null values with 'withoutgroupid'\n",
    "metadata_pd['group_id'] = metadata_pd['group_id'].astype(object)\n",
    "metadata_pd['group_id'].fillna('withoutgroupid', inplace=True)\n",
    "\n",
    "# Group files by group_id (or process individually if group_id is 'withoutgroupid')\n",
    "grouped_files = metadata_pd.groupby('group_id')['SAPFileName'].apply(list).to_dict()\n",
    "\n",
    "# Define default schema to cover cases where it's not defined\n",
    "default_schema = 'nntst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6939792b-34e1-4b82-a4ad-ffeee81af00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "def split_grouped_calc_views(input_path: str, output_dir: str):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Attempt to parse the input file\n",
    "        tree = ET.parse(input_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {input_path} not found. Please check the file path and try again.\")\n",
    "        return\n",
    "\n",
    "    # Rest of your function remains the same\n",
    "    root = tree.getroot()\n",
    "    children = list(root)\n",
    "    cv_idx = next(\n",
    "        i for i, el in enumerate(children) if el.tag.endswith(\"calculationViews\")\n",
    "    )\n",
    "    header_elems = children[:cv_idx]\n",
    "    cv_container = children[cv_idx]\n",
    "    footer_elems = children[cv_idx + 1 :]\n",
    "\n",
    "    # 2) Prepare tiktoken encoder\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # 3) Pre-serialize header & footer once\n",
    "    header_xml = \"\".join(\n",
    "        ET.tostring(el, encoding=\"utf-8\").decode(\"utf-8\") for el in header_elems\n",
    "    )\n",
    "    footer_xml = \"\".join(\n",
    "        ET.tostring(el, encoding=\"utf-8\").decode(\"utf-8\") for el in footer_elems\n",
    "    )\n",
    "    header_tokens = len(enc.encode(header_xml))\n",
    "    footer_tokens = len(enc.encode(footer_xml))\n",
    "    base_tokens = header_tokens + footer_tokens\n",
    "\n",
    "    # 4) Iterate calculationView children, grouping by token budget\n",
    "    group = []\n",
    "    group_tokens = 0\n",
    "    file_idx = 1\n",
    "    base_filename = os.path.basename(input_path)\n",
    "\n",
    "    def flush_group():\n",
    "        nonlocal group, group_tokens, file_idx\n",
    "        if not group:\n",
    "            return\n",
    "\n",
    "        # Build new tree\n",
    "        new_root = ET.Element(root.tag, root.attrib)\n",
    "        for k, v in root.attrib.items():\n",
    "            if k.startswith(\"xmlns\"):\n",
    "                new_root.set(k, v)\n",
    "\n",
    "        # Attach header\n",
    "        for el in header_elems:\n",
    "            new_root.append(copy.deepcopy(el))\n",
    "\n",
    "        # Attach grouped <calculationViews>\n",
    "        new_cv = ET.Element(cv_container.tag, cv_container.attrib)\n",
    "        for k, v in cv_container.attrib.items():\n",
    "            if k.startswith(\"xmlns\"):\n",
    "                new_cv.set(k, v)\n",
    "        for cv in group:\n",
    "            new_cv.append(copy.deepcopy(cv))\n",
    "        new_root.append(new_cv)\n",
    "\n",
    "        # Attach footer\n",
    "        for el in footer_elems:\n",
    "            new_root.append(copy.deepcopy(el))\n",
    "\n",
    "        # Write file\n",
    "        ET.indent(new_root, space=\"  \")\n",
    "        out_file = os.path.join(output_dir, f\"{base_filename}_grouped_{file_idx}.xml\")\n",
    "        ET.ElementTree(new_root).write(out_file, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "        total = base_tokens + group_tokens\n",
    "        print(\n",
    "            f\"Wrote {out_file} — header+footer: {base_tokens}, \"\n",
    "            f\"{len(group)} views: {group_tokens}, total: {total}\"\n",
    "        )\n",
    "\n",
    "        # Reset for next group\n",
    "        file_idx += 1\n",
    "        group = []\n",
    "        group_tokens = 0\n",
    "\n",
    "    # 5) Loop and accumulate\n",
    "    for single_cv in list(cv_container):\n",
    "        cv_xml = ET.tostring(single_cv, encoding=\"utf-8\").decode(\"utf-8\")\n",
    "        cv_tok = len(enc.encode(cv_xml))\n",
    "\n",
    "        # If adding this view would exceed budget, flush current group first\n",
    "        if base_tokens + group_tokens + cv_tok > MAX_TOKENS:\n",
    "            flush_group()\n",
    "\n",
    "        # Then start (or continue) a group\n",
    "        group.append(single_cv)\n",
    "        group_tokens += cv_tok\n",
    "\n",
    "    # 6) Flush any remaining views\n",
    "    flush_group()\n",
    "\n",
    "# Example usage with a full path\n",
    "input_path = \"lpdbwlpbdt01devdev.gold_vta_lego_upo_mpo.pubs_base\"\n",
    "output_dir = \"split_grouped_by_calcview\"\n",
    "split_grouped_calc_views(input_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf6a556-df22-4e30-aa52-5e13d9e10272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM hive_metastore.codeconverter_config.dataflowtableinfo\n",
    "WHERE DataBricksTableName LIKE '%PUBS%'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0301c866-c2e4-4fb5-81fc-149545ad5ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "812d4b49-57d2-45ff-83c9-bb0523a1655f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import certifi\n",
    " \n",
    "scope_name = \"OpenAI-scope\"\n",
    "secret_name = \"OpenAI-certificate\"\n",
    " \n",
    "secret_value = \"\"\"\n",
    "placeholder\n",
    "\"\"\"\n",
    " \n",
    "print(\"---- Retrieve CA ----\")\n",
    " \n",
    "ca_cert = certifi.where()\n",
    " \n",
    "print(\"---- Appending new SSL to CA ----\")\n",
    " \n",
    "with open(ca_cert, 'a') as custom_certificate:\n",
    "    cert_content = custom_certificate.write(\"\\n# Custom appended certificate \\n\")\n",
    "    cert_content = custom_certificate.write(secret_value)\n",
    "    cert_content = custom_certificate.write(\"\\n\")\n",
    " \n",
    " \n",
    "print(f\"---- Successfully appended to: {ca_cert} ----\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d252fa-fa34-4c1e-8c34-b3c6c8114ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "call_model_o1(\"o1\", \"Hello?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a3f2a55-7015-46ef-b673-d4d74fe5dfa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7971250953290444,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "calculationview_splitter",
   "widgets": {
    "MAX_TOKENS": {
     "currentValue": "120000",
     "nuid": "83a37a38-dc39-4ed2-b664-f680b9cfb7c4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "120000",
      "label": "Choose max number of tokens",
      "name": "MAX_TOKENS",
      "options": {
       "choices": [
        "150000",
        "120000",
        "100000"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "120000",
      "label": "Choose max number of tokens",
      "name": "MAX_TOKENS",
      "options": {
       "autoCreated": null,
       "choices": [
        "150000",
        "120000",
        "100000"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "data_flow_insertion": {
     "currentValue": "DF34",
     "nuid": "332b9448-4c93-4641-bc72-03947f52b27b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "DF34",
      "label": "Choose max number of tokens",
      "name": "data_flow_insertion",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "DF34",
      "label": "Choose max number of tokens",
      "name": "data_flow_insertion",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
