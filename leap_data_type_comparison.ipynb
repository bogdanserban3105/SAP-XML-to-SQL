{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3a1ecb2-5df8-4471-a75d-7678ddc432f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extract SAP VIEW | SCHEMA | COLUMN NAME | DATA TYPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1ac06e-30b3-4311-b6a7-084d9e8564a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import split, col, trim, countDistinct, collect_set, lower, when, regexp_replace, array_contains, explode, collect_list, array, explode_outer, upper\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abb7676f-6206-4dea-80a9-b892c0828360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Get mapping table information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bccedd-0bc5-4e99-b58c-ef40a90d86e0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_c0b6a5c7\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_baf52231\",\"enabled\":true,\"columnId\":\"Decoupling_View\",\"dataType\":\"string\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1747136821369}],\"syncTimestamp\":1747136821369}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_map= '''\n",
    "select * from hive_metastore.codeconverter_config.mapping_sap_dbx;\n",
    "'''\n",
    "map_df = spark.sql(query_map)\n",
    "map_df = map_df.withColumn(\"CatalogName\", split(col(\"SLO_Object\"), r\"\\{ENV\\}\")[0])\n",
    "map_df = map_df.withColumn(\"SchemaName\", split(col(\"SLO_Object\"), r\"\\.\")[1])\n",
    "map_df = map_df.withColumn(\"TableName\", split(col(\"SLO_Object\"), r\"\\.\")[2])\n",
    "\n",
    "source_package = map_df.select('source_package').distinct().collect()[0]['source_package']\n",
    "\n",
    "\n",
    "sap_databricks_lst_tup = []\n",
    "\n",
    "rows = map_df.collect()\n",
    "\n",
    "for row in rows:\n",
    "    table_id = row['TableName']\n",
    "    decoupling_view = row['Decoupling_View']\n",
    "    sap_databricks_lst_tup.append((table_id, decoupling_view))\n",
    "\n",
    "\n",
    "lst_of_cvs = [tup[1] for tup in sap_databricks_lst_tup]\n",
    "list_of_tab = [tup[0] for tup in sap_databricks_lst_tup]\n",
    "\n",
    "\n",
    "display(map_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b42c72b-764c-4855-9941-53926c4fc0cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Get SAP Object Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551328cd-3a20-46ea-92fe-30d35bffdaa9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1747222798530}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "copy_data_flag = dbutils.widgets.get(\"copy_data_from_sap\")\n",
    "\n",
    "if copy_data_flag == \"YES\":\n",
    "    \n",
    "    sap_hana_host = \"vbh1db.sap.insim.biz\"\n",
    "    sap_hana_port = \"30615\"\n",
    "    sap_hana_user = dbutils.secrets.get(scope=\"BDT mall\", key=\"SAP-HANA-username\")\n",
    "    sap_hana_password = dbutils.secrets.get(scope=\"BDT mall\", key=\"SAP-HANA-password\")\n",
    "    sap_hana_jdbc_url = f\"jdbc:sap://{sap_hana_host}:{sap_hana_port}?encrypt=true&validateCertificate=false\"\n",
    "\n",
    "    #dataflow = source_package + '%'\n",
    "    dataflow = 'NNROOT.Z00_Applications.LEGO.UPO_MPO%'\n",
    "\n",
    "\n",
    "    query_text_1 = f\"\"\"\n",
    "        SELECT DISTINCT VIEW_NAME, SCHEMA_NAME\n",
    "        FROM SYS.VIEWS\n",
    "        WHERE VIEW_NAME LIKE '{dataflow}'\n",
    "        AND VIEW_TYPE = 'CALC'\n",
    "        ORDER BY VIEW_NAME\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the first query\n",
    "    df_views = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"Driver\", \"com.sap.db.jdbc.Driver\") \\\n",
    "        .option(\"url\", sap_hana_jdbc_url) \\\n",
    "        .option(\"user\", sap_hana_user) \\\n",
    "        .option(\"password\", sap_hana_password) \\\n",
    "        .option(\"query\", query_text_1) \\\n",
    "        .load()\n",
    "\n",
    "    # Collect the results\n",
    "    view_names = df_views.select(\"VIEW_NAME\", \"SCHEMA_NAME\").distinct().collect()\n",
    "    display(view_names)\n",
    "\n",
    "    schema = StructType([StructField(\"SCHEMA_NAME\", StringType(), True), StructField(\"VIEW_NAME\", StringType(), True), StructField(\"COLUMN_NAME\", StringType(), True), \n",
    "                        StructField(\"DATA_TYPE_ID\", IntegerType(), True), StructField(\"DATA_TYPE_NAME\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Create an empty DataFrame to store the results\n",
    "    final_df = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "    # Loop through the collected results and execute the second query for each pair\n",
    "    for row in view_names:\n",
    "        view_name = row[\"VIEW_NAME\"]\n",
    "        schema_name = row[\"SCHEMA_NAME\"]\n",
    "\n",
    "        # Define the second query\n",
    "        query_text_2 = f\"\"\"\n",
    "            SELECT SCHEMA_NAME, VIEW_NAME, COLUMN_NAME, DATA_TYPE_ID, DATA_TYPE_NAME\n",
    "            FROM SYS.VIEW_COLUMNS\n",
    "            WHERE SCHEMA_NAME = '{schema_name}'\n",
    "            AND VIEW_NAME = '{view_name}'\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the second query\n",
    "        temp_df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"Driver\", \"com.sap.db.jdbc.Driver\") \\\n",
    "            .option(\"url\", sap_hana_jdbc_url) \\\n",
    "            .option(\"user\", sap_hana_user) \\\n",
    "            .option(\"password\", sap_hana_password) \\\n",
    "            .option(\"query\", query_text_2) \\\n",
    "            .load()\n",
    "\n",
    "        # # Add the VIEW_NAME and SCHEMA_NAME columns to the temp_df\n",
    "        # temp_df = temp_df.withColumn(\"VIEW_NAME\", lit(view_name))\n",
    "        # temp_df = temp_df.withColumn(\"SCHEMA_NAME\", lit(schema_name))\n",
    "\n",
    "        # Append the results to the final DataFrame\n",
    "        final_df = final_df.union(temp_df)\n",
    "\n",
    "        #dbutils.widgets.put(\"copy_data_from_sap\", \"NO\")\n",
    "\n",
    "    final_df.write.mode(\"overwrite\").saveAsTable(\"hive_metastore.codeconverter_config.sap_retention_datatype\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ee9554f-d0ac-4b46-842b-50be0483b831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Store SAP output into dbx table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982c3f9f-d677-43bb-bc25-c26593bf9a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get sap info stored in dbx\n",
    "final_df = spark.sql(\"\"\"SELECT * FROM hive_metastore.codeconverter_config.sap_retention_datatype\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5c4c83-b460-40be-bd0e-78a22f2036e7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1747228442482}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "final_df = final_df.withColumn(\"CalculationViewName\", trim(split(col(\"VIEW_NAME\"),\"/\")[1]))\n",
    "\n",
    "# Filter df on target calculation views\n",
    "final_df = final_df.filter(col(\"CalculationViewName\").isin(lst_of_cvs))\n",
    "\n",
    "no_of_unique_views = final_df.select(\"CalculationViewName\").distinct().count()\n",
    "print(f\"Number of unique calculation views: {no_of_unique_views}\")\n",
    "\n",
    "\n",
    "print(f\"Number of rows in initial SAP results: {final_df.count()}\")\n",
    "display(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0e01ca-6991-4d86-9822-f74b28e49759",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1747223504950}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Filter for unique combinations of data_type and column_name\n",
    "unique_df = final_df.dropDuplicates([ \"CalculationViewName\", \"COLUMN_NAME\", \"DATA_TYPE_NAME\"])\n",
    "\n",
    "# Select only COLUMN_NAME, DATA_TYPE_NAME, and CalculationViewName\n",
    "initial_df_filtered = unique_df.select(\"COLUMN_NAME\", \"DATA_TYPE_NAME\", \"CalculationViewName\")\n",
    "\n",
    "\n",
    "\n",
    "# Data from mapping table for filtering purpose \n",
    "sap_metadata_filter = map_df.select(['TableName', 'Decoupling_View'])\n",
    "sap_metadata_filter = sap_metadata_filter.dropDuplicates([\"TableName\", \"Decoupling_View\"])\n",
    "sap_metadata_filter = sap_metadata_filter.filter(col(\"Table_ID\") != '')\n",
    "\n",
    "\n",
    "filtered_df = initial_df_filtered.join(sap_metadata_filter, initial_df_filtered.CalculationViewName == sap_metadata_filter.Decoupling_View, \"inner\")\n",
    "\n",
    "\n",
    "# Group by COLUMN_NAME, count distinct DATA_TYPE_NAMEs, and collect DATA_TYPE_NAMEs and CalculationViewNames\n",
    "grouped_df_count = filtered_df.groupBy([\"CalculationViewName\", \"TableName\", \"COLUMN_NAME\", \"DATA_TYPE_NAME\"]).count()\n",
    "\n",
    "grouped_df =  grouped_df_count.select([\"CalculationViewName\", \"TableName\",\"COLUMN_NAME\", \"DATA_TYPE_NAME\"])\n",
    "\n",
    "\n",
    "display(grouped_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59c645ee-6a89-4fca-b84f-4e24a4b156d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# EXTRACT DATABRICKS TABLE AND VIEW INFO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e68efa2-3490-4034-b727-f363e29fbf8a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1747228416707}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "# Initialize an empty list to collect rows for DataFrame creation\n",
    "metadata_rows = []\n",
    "# Function to describe an object and collect its metadata\n",
    "def describe_object(object_type, catalog_name, schema_name, object_name):\n",
    "    describe_df = spark.sql(f\"DESCRIBE {catalog_name}.{schema_name}.{object_name}\")\n",
    "    for row in describe_df.collect():\n",
    "        column_name = row[\"col_name\"].upper()\n",
    "        data_type = row[\"data_type\"].upper()\n",
    "        metadata_rows.append(Row(object_type=object_type, catalog_name=catalog_name, schema_name=schema_name, object_name=object_name, column_name=column_name, data_type=data_type))\n",
    "\n",
    "\n",
    "# Filter out rows where CatalogName is empty or starts with \"Manual\" or \"Stable\"\n",
    "filtered_map_df = map_df.filter(\n",
    "    (col(\"CatalogName\") != '') &\n",
    "    (~col(\"CatalogName\").startswith(\"Manual\")) &\n",
    "    (~col(\"CatalogName\").startswith(\"Stable\"))\n",
    "    )\n",
    "\n",
    "# Add the suffix \"devdev\" to the CatalogName column\n",
    "filtered_map_df = filtered_map_df.withColumn(\"CatalogName\", concat(col(\"CatalogName\"), lit(\"devdev\")))\n",
    "\n",
    "\n",
    "# Collect unique catalog and schema combinations\n",
    "catalog_schema_pairs = filtered_map_df.select(\"CatalogName\", \"SchemaName\").distinct().collect()\n",
    "\n",
    "\n",
    "for pair in catalog_schema_pairs:\n",
    "    catalog_name = pair[\"CatalogName\"]\n",
    "    schema_name = pair[\"SchemaName\"]\n",
    "\n",
    "    # Process tables\n",
    "    dbx_tables = spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema_name}\")\n",
    "    table_names = list(set([row[\"tableName\"].upper() for row in dbx_tables.collect()]))  # Remove duplicates\n",
    "\n",
    "    for table in table_names:\n",
    "        describe_object('TABLE', catalog_name, schema_name, table)\n",
    "\n",
    "    # Process views\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    dbx_views = spark.sql(f\"SHOW VIEWS IN {schema_name}\")\n",
    "    view_names = list(set([row[\"viewName\"].upper() for row in dbx_views.collect()]))  # Remove duplicates\n",
    "\n",
    "    for view in view_names:\n",
    "        if view in table_names:\n",
    "            # Compare structures\n",
    "            table_structure = spark.sql(f\"DESCRIBE {catalog_name}.{schema_name}.{view}\").collect()\n",
    "            view_structure = spark.sql(f\"DESCRIBE {schema_name}.{view}\").collect()\n",
    "            if table_structure != view_structure:\n",
    "                describe_object('VIEW', catalog_name, schema_name, view)\n",
    "        else:\n",
    "            describe_object('VIEW', catalog_name, schema_name, view)\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "dbx_metadata_df = spark.createDataFrame(metadata_rows)\n",
    "\n",
    "# Show the DataFrame to inspect the results\n",
    "display(dbx_metadata_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8269843-d609-4364-8ab5-2e0a4e349dc2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1747224312595}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Filter to only one column, remove null records, and convert to uppercase\n",
    "scope_tables_df = map_df.select(\"TableName\").filter(col(\"TableName\").isNotNull()).withColumn(\"TableName\", upper(col(\"TableName\")))\n",
    "\n",
    "# Join the Databricks and SAP metadata DataFrames\n",
    "joined_df = dbx_metadata_df.join(scope_tables_df, dbx_metadata_df.object_name == scope_tables_df.TableName, \"inner\")\n",
    "\n",
    "# Remove redundant columns \n",
    "joined_df = joined_df.drop(\"TableName\")\n",
    "\n",
    "# Check number of unique tables in joined_df\n",
    "print(f\"Number of unique tables in joined_df: {joined_df.select('object_name').distinct().count()}\")\n",
    "\n",
    "# Display final dataframe \n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a5830c5-cf0a-4211-b9ba-5bd32f9add81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# COMPARE SAP AND DATABRICKS DATATYPES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee8a898-ec06-4ab0-884d-886f1e17483a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1747233952799}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, concat, lit, regexp_replace, when, coalesce\n",
    "\n",
    "# Select and rename columns from SAP metadata\n",
    "sap_metadata = grouped_df.select(\n",
    "    col(\"CalculationViewName\").alias(\"CalcViewName\"),\n",
    "    upper(col(\"TableName\")).alias(\"TableName\"),\n",
    "    col(\"COLUMN_NAME\").alias(\"ColumnNameSAP\"),\n",
    "    col(\"DATA_TYPE_NAME\").alias(\"DataTypeSAP\")\n",
    ")\n",
    "\n",
    "# Add the suffix \"devdev\" to the CatalogName column and select relevant columns from Databricks metadata\n",
    "dbx_metadata = joined_df.withColumn(\"CatalogSchema\", concat(col(\"catalog_name\"), lit(\".\"), col(\"schema_name\")))\n",
    "dbx_metadata = dbx_metadata.select(\n",
    "    col(\"object_name\").alias(\"TableNameDbx\"),\n",
    "    col(\"column_name\").alias(\"ColumnNameDbx\"),\n",
    "    col(\"data_type\").alias(\"DataTypeDbx\"),\n",
    "    col(\"CatalogSchema\").alias(\"CatalogSchema\")\n",
    ")\n",
    "\n",
    "# Perform a full outer join on both TableName and ColumnName\n",
    "merged_df = sap_metadata.join(\n",
    "    dbx_metadata,\n",
    "    (sap_metadata.TableName == dbx_metadata.TableNameDbx) & (sap_metadata.ColumnNameSAP == dbx_metadata.ColumnNameDbx),\n",
    "    \"full_outer\"\n",
    ")\n",
    "\n",
    "# Remove precision from the Databricks data type (e.g., \"DECIMAL(10,2)\" becomes \"DECIMAL\")\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"DataTypeDbx\",\n",
    "    regexp_replace(col(\"DataTypeDbx\").cast(\"string\"), r'\\(.*\\)', '')\n",
    ")\n",
    "\n",
    "# Fill null values with TableNameDbx, CalcViewName, and CatalogSchema\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"TableNameDbx\",\n",
    "    coalesce(col(\"TableNameDbx\"), col(\"TableName\"))\n",
    ").withColumn(\n",
    "    \"CalcViewName\",\n",
    "    coalesce(col(\"CalcViewName\"), col(\"CalcViewName\"))\n",
    ").withColumn(\n",
    "    \"CatalogSchema\",\n",
    "    coalesce(col(\"CatalogSchema\"), col(\"CatalogSchema\"))\n",
    ")\n",
    "\n",
    "# Determine if the data types match\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"DataTypeMatch\",\n",
    "    when(\n",
    "        (col(\"DataTypeDbx\").isNull() | col(\"DataTypeSAP\").isNull()), \n",
    "        None\n",
    "    ).when(\n",
    "        (col(\"DataTypeDbx\") == 'STRING') & (col('DataTypeSAP') == 'NVARCHAR'), \n",
    "        True\n",
    "    ).otherwise(col(\"DataTypeDbx\") == col(\"DataTypeSAP\"))\n",
    ")\n",
    "\n",
    "# Select the final columns to display, including all columns from both sources\n",
    "final_df = merged_df.select(\n",
    "    'CatalogSchema', \n",
    "    'TableNameDbx', \n",
    "    'CalcViewName', \n",
    "    'ColumnNameDbx', \n",
    "    'ColumnNameSAP', \n",
    "    'DataTypeDbx', \n",
    "    'DataTypeSAP', \n",
    "    'DataTypeMatch'\n",
    ")\n",
    "\n",
    "# Display the final DataFrame\n",
    "display(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "026b8f3b-2789-402f-85ba-f63d6253862b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (  \n",
    "    col, upper, concat, lit, regexp_replace, when, coalesce  \n",
    ")  \n",
    "from pyspark.sql.types import StringType  \n",
    "from pyspark.sql import functions as F  \n",
    "  \n",
    "# --- 1. Prepare SAP Metadata ---  \n",
    "sap_metadata = grouped_df.select(  \n",
    "    col(\"CalculationViewName\").alias(\"CalcViewName\"),  \n",
    "    upper(col(\"TableName\")).alias(\"TableName\"),  \n",
    "    col(\"COLUMN_NAME\").alias(\"ColumnNameSAP\"),  \n",
    "    col(\"DATA_TYPE_NAME\").alias(\"DataTypeSAP\")  \n",
    ")  \n",
    "  \n",
    "# --- 2. Prepare Databricks Metadata ---  \n",
    "dbx_metadata = joined_df.withColumn(  \n",
    "    \"CatalogSchema\", concat(col(\"catalog_name\"), lit(\".\"), col(\"schema_name\"))  \n",
    ").select(  \n",
    "    col(\"object_name\").alias(\"TableNameDbx\"),  \n",
    "    col(\"column_name\").alias(\"ColumnNameDbx\"),  \n",
    "    col(\"data_type\").alias(\"DataTypeDbx\"),  \n",
    "    col(\"CatalogSchema\").alias(\"CatalogSchema\")  \n",
    ")  \n",
    "  \n",
    "# --- 3. Full Outer Join on Table Name and Column Name ---  \n",
    "merged_df = sap_metadata.join(  \n",
    "    dbx_metadata,  \n",
    "    (sap_metadata.TableName == dbx_metadata.TableNameDbx) &   \n",
    "    (sap_metadata.ColumnNameSAP == dbx_metadata.ColumnNameDbx),  \n",
    "    \"full_outer\"  \n",
    ")  \n",
    "  \n",
    "# --- 4. Remove precision from Databricks data type (e.g., DECIMAL(10,2) -> DECIMAL) ---  \n",
    "merged_df = merged_df.withColumn(  \n",
    "    \"DataTypeDbx\",  \n",
    "    regexp_replace(col(\"DataTypeDbx\").cast(\"string\"), r'\\(.*\\)', '')  \n",
    ")  \n",
    "  \n",
    "# --- 5. Fill null values with values from the other source ---  \n",
    "merged_df = merged_df.withColumn(  \n",
    "    \"TableNameDbx\",  \n",
    "    coalesce(col(\"TableNameDbx\"), col(\"TableName\"))  \n",
    ").withColumn(  \n",
    "    \"CalcViewName\",  \n",
    "    coalesce(col(\"CalcViewName\"), col(\"CalcViewName\"))  # No change; could perhaps drop  \n",
    ").withColumn(  \n",
    "    \"CatalogSchema\",  \n",
    "    coalesce(col(\"CatalogSchema\"), col(\"CatalogSchema\"))  # No change; could perhaps drop  \n",
    ")  \n",
    "  \n",
    "# --- 6. Determine if the data types match ---  \n",
    "merged_df = merged_df.withColumn(  \n",
    "    \"DataTypeMatch\",  \n",
    "    when(  \n",
    "        (col(\"DataTypeDbx\").isNull() | col(\"DataTypeSAP\").isNull()), None  \n",
    "    ).when(  \n",
    "        (col(\"DataTypeDbx\") == 'STRING') & (col('DataTypeSAP') == 'NVARCHAR'), True  \n",
    "    ).otherwise(col(\"DataTypeDbx\") == col(\"DataTypeSAP\"))  \n",
    ")  \n",
    "  \n",
    "# --- 7. Select final columns (for clarity and next processing) ---  \n",
    "final_df = merged_df.select(  \n",
    "    'CatalogSchema',  \n",
    "    'TableNameDbx',  \n",
    "    'CalcViewName',  \n",
    "    'ColumnNameDbx',  \n",
    "    'ColumnNameSAP',  \n",
    "    'DataTypeDbx',  \n",
    "    'DataTypeSAP',  \n",
    "    'DataTypeMatch'  \n",
    ")  \n",
    "  \n",
    "# --- 8. Collect all unique non-null ColumnNameDbx values as a list on the driver ---  \n",
    "col_names_dbx = [  \n",
    "    row['ColumnNameDbx']  \n",
    "    for row in final_df.select('ColumnNameDbx').distinct().na.drop().collect()  \n",
    "]  \n",
    "  \n",
    "# --- 9. Define a UDF to find the first match (substring search) ---  \n",
    "def find_matching_dbx(sap_col):  \n",
    "    if sap_col is None:  \n",
    "        return None  \n",
    "    for dbx in col_names_dbx:  \n",
    "        if sap_col in dbx:  \n",
    "            return dbx  #\n",
    "    return None\n",
    "  \n",
    "find_matching_dbx_udf = F.udf(find_matching_dbx, StringType())\n",
    "\n",
    "final_df =  final_df.withColumn(\"MatchedCOlumnDbx\", \n",
    "                                    when(\n",
    "                                        col(\"ColumnNameDbx\").isNull(), \n",
    "                                        find_matching_dbx_udf(col(\"ColumnNameSAP\"))\n",
    "                                    ).otherwise(None)\n",
    "                                    )\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "451256e0-65a3-4356-ac76-d3035c64f641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType  \n",
    "\n",
    "\n",
    "# Step 1: Clean data type and create lookup dict  \n",
    "dbx_metadata_clean = dbx_metadata.withColumn(  \n",
    "    \"DataTypeDbx\",  \n",
    "    regexp_replace(col(\"DataTypeDbx\").cast(\"string\"), r'\\(.*\\)', '')  \n",
    ")  \n",
    "dbx_coltype_dict = {  \n",
    "    (row.TableNameDbx, row.ColumnNameDbx): row.DataTypeDbx  \n",
    "    for row in dbx_metadata_clean.select(\"TableNameDbx\", \"ColumnNameDbx\", \"DataTypeDbx\").distinct().collect()  \n",
    "}  \n",
    "  \n",
    "# Step 2: UDF for lookup  \n",
    "def lookup_matched_datatype(tablename, matched_col):  \n",
    "    if not tablename or not matched_col:  \n",
    "        return None  \n",
    "    return dbx_coltype_dict.get((tablename, matched_col), None)  \n",
    "  \n",
    "  \n",
    "lookup_matched_datatype_udf = F.udf(lookup_matched_datatype, StringType())  \n",
    "  \n",
    "# Step 3: Add the new column 'DataTypeDbxMatched' for the matched Databricks column's data type  \n",
    "final_df = final_df.withColumn(  \n",
    "    \"DataTypeDbxMatched\",  \n",
    "    lookup_matched_datatype_udf(  \n",
    "        col(\"TableNameDbx\"),  \n",
    "        col(\"MatchedCOlumnDbx\")   # Note: ensure your capitalization matches your actual column name!  \n",
    "    )  \n",
    ")  \n",
    "  \n",
    "# Step 4: Compare with SAP data type, store the result in 'DataTypeMatchAfterMatch'  \n",
    "final_df = final_df.withColumn(  \n",
    "    \"DataTypeMatchAfterMatch\",  \n",
    "    when(  \n",
    "        col(\"DataTypeDbxMatched\").isNull() | col(\"DataTypeSAP\").isNull(), None  \n",
    "    ).when(  \n",
    "        (col(\"DataTypeDbxMatched\") == 'STRING') & (col('DataTypeSAP') == 'NVARCHAR'), True  \n",
    "    ).otherwise(col(\"DataTypeDbxMatched\") == col(\"DataTypeSAP\"))  \n",
    ")  \n",
    "  \n",
    "# Step 5: Show the results (or write for further use)  \n",
    "final_df.select(  \n",
    "    'CatalogSchema',  \n",
    "    'TableNameDbx',  \n",
    "    'CalcViewName',  \n",
    "    'ColumnNameDbx',  \n",
    "    'ColumnNameSAP',  \n",
    "    'MatchedCOlumnDbx',  \n",
    "    'DataTypeSAP',  \n",
    "    'DataTypeDbx',  \n",
    "    'DataTypeDbxMatched',  \n",
    "    'DataTypeMatchAfterMatch'  \n",
    ")\n",
    "\n",
    "display(final_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4578626400164031,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "leap_data_type_comparison",
   "widgets": {
    "copy_data_from_sap": {
     "currentValue": "YES",
     "nuid": "b26d7879-9c07-46c7-a296-695b1bfc58af",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "YES",
      "label": "Copy Data From SAP",
      "name": "copy_data_from_sap",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "YES",
        "NO"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "YES",
      "label": "Copy Data From SAP",
      "name": "copy_data_from_sap",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "YES",
        "NO"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
