{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d2ec8c-d0f1-474f-a4bb-cfba4b26be75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SAP to Databricks Code Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f2cb03-8eab-49d3-8376-6ab5a246b776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# How to use:\n",
    "Temporary until SQL read from Hana is in place. \n",
    "1. Export XMLs from Hana\n",
    "2.Open blob storage and save files in \n",
    "Blob container->nndemo->AcceleratorSAPFiles->InputFiles\n",
    "3. Select DF from Widget on top of the page: Select directory. \n",
    "4. Run script to read uploaded file into config table: extract_filename_description_from_xml_v2:\n",
    "5. Update the Config table if needed. (use cell below)\n",
    "6. Run  all below\n",
    "Result:\n",
    "Result files will be automatically posted in Blob container->nndemo->AcceleratorSAPFiles-> Validated files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946062c0-b83f-41ee-af57-580d5fe5e4bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Config File maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7607d87a-249e-4906-a788-8391f1d9a49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Table below is used for setting where the files are stored and where the prompts are stored. usually no change needed unless there are new flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e75d53-3132-443a-aa30-623033a2c077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from codeconverter_config.dataflowbaseinfo\n",
    "--if there is no entry for the DF then use the insert below\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13acd11e-d9f0-495c-96e2-4b1c84a06bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The table below is used to store the list of all objects per data flow. Maintenance can be done in order to:\n",
    "- group the CV together. \n",
    "- rename CVs\n",
    "Use this to maintain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1af8c15-4034-4202-9642-c0ef1a82661b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from codeconverter_config.dataflowtableinfo where DataFlow = 'DF34'\n",
    "--where DataFlow = 'DF34'\n",
    "\n",
    "-- for maintenance run the below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406c9abe-ad04-42e0-8a28-7d44d2ce512d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import required modules (libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374e846e-9762-4753-b660-285263ec78a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required modules (libraries)\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974a92c8-f27f-45d3-b72b-e7a8a7230656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # This will allow widget creation in the source notebook\n",
    "is_target_notebook = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8754bbb-50f2-4d22-b1ba-f05d6107c45b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc28c27-c16f-4e02-a316-135453f8ab37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a44bd30-2be0-43ec-91e7-ef8f391e3296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to call the appropriate model based on selected type\n",
    "def call_model(selected_model, prompt):\n",
    "    if selected_model == \"o1\":\n",
    "        return call_model_o1(deployment_name=\"o1\", prompt=prompt)  # Call the o1 model function\n",
    "    elif selected_model == \"o1-preview\":\n",
    "        return callmodelo1(deployment_name=\"o1-preview\", prompt=prompt)  # Call the o1-preview model function\n",
    "    elif selected_model == \"o1-mini\":\n",
    "        return callmodelo1(deployment_name=\"o1-mini\", prompt=prompt)  # Call the o1-mini model function\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model choice\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ae8f93-5641-47f9-a636-730a144d4865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get parameter value(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9430fb83-bf8e-485e-a28d-dace96c349e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a dropdown widget for selecting model type (o1 or o1 mini)\n",
    "dbutils.widgets.dropdown(\"model_choice\", \"o1\", [\"o1\", \"o1-mini\", \"o1-preview\"], \"Choose model type\")\n",
    "# Retrieve the selected model type\n",
    "selected_model = dbutils.widgets.get(\"model_choice\")\n",
    "print(f\"Selected model: {selected_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e501f3-1f8c-457b-af45-5913f1d13899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get parameters value\n",
    "data_flow_man = dbutils.widgets.get(\"data_flow_insertion\")\n",
    "data_flow_man = data_flow_man.upper()\n",
    "print(f\"Current working directory: {data_flow_man}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a59ca5d-2917-4816-9877-2f671f007c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Set your storage account SAS token and name\n",
    "# sasToken = dbutils.secrets.get(scope=\"codeconv\", key=\"sasTokenKey\") # Create a new scope (directly from databricks UI) and secret (in Key-Vault)\n",
    "# #sa = \"erdccalearning\" # Replace with your actual storage account name\n",
    "# sa = dbutils.secrets.get(scope=\"codeconv\", key=\"storageAccountName\")\n",
    "# container_name = \"nndemo\" # Replace with your actual container name\n",
    "# spark.conf.set(f\"fs.azure.account.auth.type.{sa}.dfs.core.windows.net\", \"SAS\")\n",
    "# spark.conf.set(f\"fs.azure.sas.token.provider.type.{sa}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "# spark.conf.set(f\"fs.azure.sas.nndemo.{sa}.blob.core.windows.net\", f\"{sasToken}\")\n",
    "\n",
    "sa = \"stlpdel01dev\"\n",
    "container_name = \"codeconverter\" # Replace with your actual container name\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"CustomAccessToken\")\n",
    "spark.conf.set(\"fs.azure.account.custom.token.provider.class\", spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d8296a-c876-45b6-86c9-05cc6d47c19a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get file info from config table (DataFlowTableInfo)\n",
    "get_dataflowbaseinfo = f\"\"\"\n",
    "    SELECT * FROM codeconverter_config.dataflowbaseinfo\n",
    "    WHERE DataFlow = '{data_flow_man}'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and assign to a DataFrame\n",
    "result_flowbaseinfo = spark.sql(get_dataflowbaseinfo)\n",
    "\n",
    "# Get values from DataFlowBaseInfo table\n",
    "dataflow_value = result_flowbaseinfo.select('DataFlow').first()[0]\n",
    "dataflow_files_sourcepath_value = result_flowbaseinfo.select('DataFlowFilesSourceFolder').first()[0]\n",
    "prompt_value = result_flowbaseinfo.select('PathToPrompt').first()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17dd2563-3156-4a1b-a47a-430827e0a987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"DataFlow: \" + dataflow_value, \"Source Path: \" + dataflow_files_sourcepath_value, \"Prompt Path: \" + prompt_value, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11aa78b9-75cf-47fe-9a1b-872537c8acb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the base URL for the container and directories\n",
    "base_url = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/\"\n",
    "\n",
    "# Set the source directory path based on the selected folder\n",
    "source_directory_path = f\"{base_url}{dataflow_files_sourcepath_value}/\"\n",
    "\n",
    "# Create subdirectories under ValidatedFiles, ArchivedFiles, and Logs using the data_flow_sel\n",
    "target_directory_path = f\"{base_url}ValidatedFiles/{data_flow_man}/\"\n",
    "archive_directory_path = f\"{base_url}ArchivedFiles/{data_flow_man}/\"\n",
    "\n",
    "# Create an archive folder with the current date\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "archive_date_folder = f\"{archive_directory_path.rstrip('/')}/{current_date}/\"\n",
    "dbutils.fs.mkdirs(archive_date_folder)\n",
    "\n",
    "# Create a logs directory in the Blob Storage container\n",
    "logs_directory_path = f\"{base_url}Logs/{data_flow_man}/{current_date}/\"\n",
    "dbutils.fs.mkdirs(logs_directory_path)\n",
    "\n",
    "# Define Prompt Path\n",
    "prompt_path = base_url + prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b10739cd-5d6b-46a2-88ea-32664504e1bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Prompt Path: \" + prompt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1fcb2da-24d0-47b5-a65e-25bc7855e436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "054166fb-0d68-445e-9f7f-e6f8d0b29711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = read_file_content(prompt_path)\n",
    "detailed_prompt = '\\n'.join(prompt) # can also use prompt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db76b30-c877-4868-8677-3bd6e4f3d14b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23eec41-3b6d-45fd-b4f5-2a1c9111552d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loop through CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e597070d-91fe-456b-9968-d5c254e0d4f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List files in the source directory\n",
    "files = dbutils.fs.ls(source_directory_path)\n",
    "\n",
    "# Retrieve the HANA-related files\n",
    "hana_files_query = f\"SELECT SAPFileName FROM codeconverter_config.DataFlowTableInfo WHERE DataFlow = '{data_flow_man}' AND SourceSystem = 'HANA'\"\n",
    "hana_files_df = spark.sql(hana_files_query)\n",
    "\n",
    "# Convert the result into a list of file names\n",
    "hana_files = [row['SAPFileName'] for row in hana_files_df.collect()]\n",
    "\n",
    "# Keep only files with source system HANA\n",
    "files = [file for file in files if file.name in hana_files]\n",
    "\n",
    "# Initialize a list to store all log messages\n",
    "log_messages = []\n",
    "\n",
    "# Configuration query\n",
    "get_dataflowtableinfo = f\"\"\"\n",
    "SELECT * FROM codeconverter_config.DataFlowTableInfo\n",
    "WHERE DataFlow = '{data_flow_man}'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and convert the results to Pandas DataFrame\n",
    "result_flowtableinfo = spark.sql(get_dataflowtableinfo)\n",
    "metadata_pd = result_flowtableinfo.toPandas()\n",
    "\n",
    "# Cast 'group_id' column to object dtype and fill null values with 'withoutgroupid'\n",
    "metadata_pd['group_id'] = metadata_pd['group_id'].astype(object)\n",
    "metadata_pd['group_id'].fillna('withoutgroupid', inplace=True)\n",
    "\n",
    "# Group files by group_id (or process individually if group_id is 'withoutgroupid')\n",
    "grouped_files = metadata_pd.groupby('group_id')['SAPFileName'].apply(list).to_dict()\n",
    "\n",
    "# Define default schema to cover cases where it's not defined\n",
    "default_schema = 'nntst'\n",
    "\n",
    "# Function to process each file or group of files\n",
    "def process_file_or_group(group_id, available_file_list):\n",
    "    iteration_log = []\n",
    "    iteration_log.append(f\"**Start Processing** for Group {group_id}\\n\")\n",
    "    \n",
    "    try:\n",
    "        if not available_file_list:\n",
    "            iteration_log.append(f\"No files found for Group {group_id}\\n\")\n",
    "            return iteration_log\n",
    "\n",
    "        # Log message if the number of files being processed is less than the expected\n",
    "        if len(available_file_list) < len(grouped_files[group_id]):\n",
    "            log_message = f\"Only {len(available_file_list)} out of {len(grouped_files[group_id])} files found and processed from the blob storage for Group {group_id}: {', '.join(available_file_list)}\"\n",
    "            log_to_blob(log_message, \"Processing\")\n",
    "            iteration_log.append(f\"**Processing Info**: {log_message}\\n\")\n",
    "\n",
    "        if group_id == 'withoutgroupid':\n",
    "            for file in available_file_list:\n",
    "                file_path = next((f.path for f in files if f.name == file), None)\n",
    "                if file_path:\n",
    "                    iteration_log.append(f\"**Start Processing** for file {file}\\n\")\n",
    "                    log_message = f\"Reading file: {file_path}\"\n",
    "                    log_to_blob(log_message, \"Read\")\n",
    "                    iteration_log.append(f\"Reading file: {file_path}\\n\")\n",
    "                    try:\n",
    "                        file_content = read_file_content(file_path)\n",
    "                        if file_content is None:\n",
    "                            continue\n",
    "                        file_content_as_string = \"\\n\".join(file_content)\n",
    "                        file_metadata = metadata_pd[metadata_pd['SAPFileName'] == file].iloc[0]\n",
    "                        schema = file_metadata['SchemaName'] if not pd.isnull(file_metadata['SchemaName']) else default_schema\n",
    "                        table = file_metadata['DataBricksTableName']\n",
    "                        start_time_initial = time.time()\n",
    "                        prompt = f\"{detailed_prompt}. Take into account to use the schema '{schema}' and table '{table}':\\n{file_content_as_string}\"\n",
    "                        model_output = call_model(selected_model, prompt=prompt)\n",
    "                        end_time_initial = time.time()\n",
    "                        initial_conversion_time = end_time_initial - start_time_initial\n",
    "                        log_message = f\"Initial model conversion for file {file} took {initial_conversion_time:.2f} seconds\"\n",
    "                        log_to_blob(log_message, \"Initial Conversion\")\n",
    "                        iteration_log.append(f\"**Initial Conversion** took {initial_conversion_time:.2f} seconds for file {file}\\n\")\n",
    "                        reassess_prompt = f\"Please confirm that the following translation of SAP code to Databricks SQL code using the schema '{schema}' and table '{table}' is correct. \"\n",
    "                        reassess_prompt += f\"If necessary, improve the translation, but make sure to **provide the corrected Databricks SQL code** at the end:\\n{file_content_as_string}\\n\\n\"\n",
    "                        reassess_prompt += f\"Initial Databricks SQL code:\\n{model_output}\\n\\n\"\n",
    "                        reassess_prompt += \"Please review and provide the **final SQL code** as the output, keeping the language cast to SQL.\"\n",
    "                        reassess_prompt += \"Keep the documentation/comments generated previously, correct any errors in the descriptions, and update them as needed to reflect any changes or new information. Ensure the documentation remains accurate, clear, and up-to-date based on the latest changes.\"\n",
    "                        reassessed_output = call_model(selected_model, prompt=reassess_prompt)\n",
    "                        output_file_path = f\"{target_directory_path}{os.path.basename(file_path).replace('.txt', '_validated_code.txt')}\"\n",
    "                        dbutils.fs.put(output_file_path, reassessed_output, overwrite=True)\n",
    "                        log_message = f\"Successfully processed and saved validated output for file {file}\"\n",
    "                        log_to_blob(log_message, \"Validated Output\")\n",
    "                        iteration_log.append(f\"**Validated Output**: Successfully processed and saved for file {file}\\n\")\n",
    "                        archive_file_path = f\"{archive_date_folder}{file}\"\n",
    "                        dbutils.fs.mv(file_path, archive_file_path)\n",
    "                        log_message = f\"Successfully archived the original file {file} to: {archive_file_path}\"\n",
    "                        log_to_blob(log_message, \"Archive\")\n",
    "                        iteration_log.append(f\"**Archive**: Successfully archived the original file {file}\\n\")\n",
    "                    except Exception as e:\n",
    "                        log_message = f\"Error processing file {file}: {str(e)}\"\n",
    "                        log_to_blob(log_message, \"Error\")\n",
    "                        iteration_log.append(f\"**Error**: {log_message}\\n\")\n",
    "                    current_time = datetime.now().strftime('%H-%M-%S')\n",
    "                    iteration_log_filename = f\"log_{os.path.basename(file_path).replace('.txt', '')}_{current_date}_{current_time}.log\"\n",
    "                    iteration_log_file_path = f\"{logs_directory_path}{iteration_log_filename}\"\n",
    "                    try:\n",
    "                        dbutils.fs.put(iteration_log_file_path, \"\\n\".join(iteration_log), overwrite=True)\n",
    "                        print(f\"Iteration log written for file {file} to: {iteration_log_file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error writing iteration log for file {file}: {e}\")\n",
    "        else:\n",
    "            combined_content = []\n",
    "            combined_file_names = []\n",
    "            sorted_file_list = sorted(available_file_list)\n",
    "            for file in sorted_file_list:\n",
    "                file_path = next((f.path for f in files if f.name == file), None)\n",
    "                if file_path:\n",
    "                    log_message = f\"Reading file: {file_path}\"\n",
    "                    log_to_blob(log_message, \"Read\")\n",
    "                    iteration_log.append(f\"Reading file: {file_path}\\n\")\n",
    "                    file_content = read_file_content(file_path)\n",
    "                    if file_content is None:\n",
    "                        continue\n",
    "                    combined_file_names.append(os.path.basename(file_path).replace(\".txt\", \"\"))\n",
    "                    combined_content.append(f\"CV_{combined_file_names[-1]} starts here\\n\")\n",
    "                    combined_content.extend(file_content)\n",
    "            if len(sorted_file_list) > 1:\n",
    "                final_parts = [name.split('.')[0].split('_')[-2:] for name in combined_file_names]\n",
    "                combined_filename_parts = [part for sublist in final_parts for part in sublist]\n",
    "                combined_filename = \"CV_\" + \"_\".join(combined_filename_parts)\n",
    "            else:\n",
    "                combined_filename = \"CV_\" + \"_\".join(combined_file_names[0].split('.')[0].split('_')[-2:])\n",
    "            combined_content_as_string = \"\\n\".join(combined_content)\n",
    "            group_metadata = metadata_pd[metadata_pd['group_id'] == group_id].iloc[0]\n",
    "            schema = group_metadata['SchemaName'] if not pd.isnull(group_metadata['SchemaName']) else default_schema\n",
    "            table = group_metadata['DataBricksTableName']\n",
    "            start_time_initial = time.time()\n",
    "            prompt = f\"{detailed_prompt}. Take into account to use the schema '{schema}' and table '{table}':\\n{combined_content_as_string}\"\n",
    "            model_output = call_model(selected_model, prompt=prompt)\n",
    "            end_time_initial = time.time()\n",
    "            initial_conversion_time = end_time_initial - start_time_initial\n",
    "            log_message = f\"Initial model conversion for Group {group_id} took {initial_conversion_time:.2f} seconds\"\n",
    "            log_to_blob(log_message, \"Initial Conversion\")\n",
    "            iteration_log.append(f\"**Initial Conversion** took {initial_conversion_time:.2f} seconds for Group {group_id}\\n\")\n",
    "            reassess_prompt = f\"Please confirm that the following translation of SAP code to Databricks SQL code using the schema '{schema}' and table '{table}' is correct. \"\n",
    "            reassess_prompt += f\"If necessary, improve the translation, but make sure to **provide the corrected Databricks SQL code** at the end:\\n{combined_content_as_string}\\n\\n\"\n",
    "            reassess_prompt += f\"Initial Databricks SQL code:\\n{model_output}\\n\\n\"\n",
    "            reassess_prompt += \"Please review and provide the **final SQL code** as the output, keeping the language cast to SQL.\"\n",
    "            reassess_prompt += \"Keep the documentation/comments generated previously, correct any errors in the descriptions, and update them as needed to reflect any changes or new information. Ensure the documentation remains accurate, clear, and up-to-date based on the latest changes.\"\n",
    "            reassessed_output = call_model(selected_model, prompt=reassess_prompt)\n",
    "            output_file_path = f\"{target_directory_path}{combined_filename}.validated_code.txt\"\n",
    "            dbutils.fs.put(output_file_path, reassessed_output, overwrite=True)\n",
    "            log_message = f\"Successfully processed and saved validated output for Group {group_id}\"\n",
    "            log_to_blob(log_message, \"Validated Output\")\n",
    "            iteration_log.append(f\"**Validated Output**: Successfully processed and saved for Group {group_id}\\n\")\n",
    "            for file in available_file_list:\n",
    "                file_path = next((f.path for f in files if f.name == file), None)\n",
    "                if file_path:\n",
    "                    archive_file_path = f\"{archive_date_folder}{file}\"\n",
    "                    dbutils.fs.mv(file_path, archive_file_path)\n",
    "                    log_message = f\"Successfully archived the original file {file} to: {archive_file_path}\"\n",
    "                    log_to_blob(log_message, \"Archive\")\n",
    "                    iteration_log.append(f\"**Archive**: Successfully archived the original file {file}\\n\")\n",
    "            iteration_log_filename = f\"log_group{group_id}_{current_date}.log\"\n",
    "            iteration_log_file_path = f\"{logs_directory_path}{iteration_log_filename}\"\n",
    "            try:\n",
    "                dbutils.fs.put(iteration_log_file_path, \"\\n\".join(iteration_log), overwrite=True)\n",
    "                print(f\"Iteration log written for Group {group_id} to: {iteration_log_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing iteration log for Group {group_id}: {e}\")\n",
    "    except Exception as e:\n",
    "        log_message = f\"Error processing Group {group_id}: {str(e)}\"\n",
    "        log_to_blob(log_message, \"Error\")\n",
    "        iteration_log.append(f\"**Error**: {log_message}\\n\")\n",
    "    return iteration_log\n",
    "\n",
    "\n",
    "# Submit tasks for processing only available files\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future_to_group = {\n",
    "        executor.submit(process_file_or_group, group_id, available_file_list): group_id\n",
    "        for group_id, file_list in grouped_files.items()\n",
    "        for available_file_list in [\n",
    "            [f for f in file_list if any(f == blob_file.name for blob_file in files)]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Wait for all futures to complete and process results\n",
    "    for future in concurrent.futures.as_completed(future_to_group):\n",
    "        group_id = future_to_group[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            log_messages.extend(result)\n",
    "        except Exception as e:\n",
    "            log_messages.append(f\"Error processing Group {group_id}: {e}\\n\")\n",
    "\n",
    "# The `with` block ensures that the executor is properly shut down after the tasks are completed.\n",
    "current_time = datetime.now().strftime('%H-%M-%S')\n",
    "final_log_filename = f\"final_log_{current_time}.log\"\n",
    "final_log_file_path = f\"{logs_directory_path}{final_log_filename}\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.put(final_log_file_path, \"\\n\".join(log_messages), overwrite=True)\n",
    "    print(f\"Final log written to: {final_log_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing final log: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1603391135375773,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CodeConvertFromXMLtoSQL_v2",
   "widgets": {
    "data_flow_insertion": {
     "currentValue": "DF34",
     "nuid": "362e2fdc-feb3-4f00-8117-091af593cdd2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Directory Name (e.g., DF1)",
      "name": "data_flow_insertion",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "Enter Directory Name (e.g., DF1)",
      "name": "data_flow_insertion",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "model_choice": {
     "currentValue": "o1",
     "nuid": "b0450d8b-65f0-474a-ba2e-bc0e31e4e7be",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "o1",
      "label": "Choose model type",
      "name": "model_choice",
      "options": {
       "choices": [
        "o1",
        "o1-mini",
        "o1-preview"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "o1",
      "label": "Choose model type",
      "name": "model_choice",
      "options": {
       "autoCreated": null,
       "choices": [
        "o1",
        "o1-mini",
        "o1-preview"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
