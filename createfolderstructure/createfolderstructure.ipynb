{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9b32f4-2dbb-4327-a5e9-cf8555c27e7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required modules (libraries)\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d30ad94a-3e96-444f-9f87-6920e2da4e5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use SAS Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "653d31d2-6e78-4d75-9b62-6e1913b0ba01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set your storage account SAS token and name\n",
    "sasToken = dbutils.secrets.get(scope=\"OpenAI-scope\", key=\"Deloitte-Storage-SAS-Token\") # Create a new scope (directly from databricks UI) and secret (in Key-Vault)\n",
    "#sa = \"erdccalearning\" # Replace with your actual storage account name\n",
    "sa = \"stlpdel01dev\"\n",
    "container_name = \"codeconverter\" # Replace with your actual container name\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{sa}.dfs.core.windows.net\", \"SAS\")\n",
    "#spark.conf.set(\"fs.azure.sas.token.provider.type.{sa}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token\", f\"{sasToken}\")\n",
    "spark.conf.set(f\"fs.azure.sas.{container_name}.{sa}.dfs.core.windows.net\", f\"{sasToken}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84e36ad4-ef06-42c8-a255-7f5cae14fc16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use Pass-Through Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7a3a84-e27c-45d4-bcda-daa5534e5845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sa = \"stlpdel01dev\"\n",
    "container_name = \"codeconverter\" # Replace with your actual container name\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"CustomAccessToken\")\n",
    "spark.conf.set(\"fs.azure.account.custom.token.provider.class\", spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a6d853-8386-4cd5-9334-c110e99dab93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/Lineage_SAP/input/\"\n",
    "#Create DataFlow directories, DF1 to DF48\n",
    "dbutils.fs.ls(base_path)\n",
    "\n",
    "for i in range(1, 49):\n",
    "    directory_name = f\"DF{i}\"\n",
    "    full_path = os.path.join(base_path, directory_name) # Create full path for the directory\n",
    "    #Use dbutils to create the directory\n",
    "    dbutils.fs.mkdirs(full_path)\n",
    "    print(f\"Created directory: {full_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e64325f5-8f0c-4ed3-97e4-044a7cd45683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secrets = dbutils.secrets.list(\"OpenAI-scope\")\n",
    "for secret in secrets:\n",
    "    print(secret)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "createfolderstructure",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
