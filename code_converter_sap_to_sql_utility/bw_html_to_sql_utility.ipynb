{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe926c16-9c46-4f46-a2b9-c2a0e978ce87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fdc265a-2beb-401f-a396-43437b72d4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "import time\n",
    "import lxml\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1847786-e99d-4ebf-a6b3-9c7f7a30d786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a dropdown widget for selecting model type (o1 or o1 mini)\n",
    "dbutils.widgets.dropdown(\"model_choice\", \"o1\", [\"o1\", \"o1-mini\", \"o1-preview\"], \"Choose model type\")\n",
    "# Retrieve the selected model type\n",
    "selected_model = dbutils.widgets.get(\"model_choice\")\n",
    "print(f\"Selected model: {selected_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8216c781-4d82-4c82-be15-660623cf7e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to call the appropriate model based on selected type\n",
    "def call_model(selected_model, prompt):\n",
    "    if selected_model == \"o1\":\n",
    "        return call_model_o1(deployment_name=\"o1\", prompt=prompt)  # Call the o1 model function\n",
    "    elif selected_model == \"o1-preview\":\n",
    "        return callmodelo1(deployment_name=\"o1-preview\", prompt=prompt)  # Call the o1-preview model function\n",
    "    elif selected_model == \"o1-mini\":\n",
    "        return callmodelo1(deployment_name=\"o1-mini\", prompt=prompt)  # Call the o1-mini model function\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc23a1f0-502b-4fb1-be75-d4598329786f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_of_filtered_trans =['01DQ1YT1TSXD859H6OJ6Y69PN4ZVNZQX'] #['087DSWSR0WU60CWHZ9OEEZB5N1J9RAK8', '048S2CO3DJUJ0RSSGU6N8HM2V22QF1RE', '0KVSPP1Y4QU15UUF4BS9UOOKJXJT32RN','0STEEWOV3FBPK2SKQUCOSZW6O8M4QLKP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a84260d-1441-434f-9f0a-b129eff02ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install html_form_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd54586-9b56-48fa-924a-becec50aa211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import html_form_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bf70c5-3299-4b68-a2fd-4a110b964976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "for subdir, _, files in os.walk(\"html/\"):\n",
    "    for file in files:\n",
    "        HTML_FILE = subdir + file\n",
    "        base_url = subdir\n",
    "        input_directory_base_path = subdir\n",
    "\n",
    "# Define functions\n",
    "def process_html_spark(file_path):\n",
    "    # Read the HTML file as plain text into a DataFrame\n",
    "    df = spark.read.text(f\"file:{os.getcwd()}/{file_path}\")\n",
    "    # Combine all lines of the HTML file into one string\n",
    "    html = \"\\n\".join([row['value'] for row in df.collect()])\n",
    "    # Remove everything inside <HEAD>...</HEAD>\n",
    "    html = re.sub(r'<HEAD>.*?</HEAD>', '', html, flags=re.DOTALL | re.IGNORECASE)\n",
    "    # Remove all <img ...> tags\n",
    "    html = re.sub(r'<img([\\w\\W]+?)>', '', html, flags=re.IGNORECASE)\n",
    "    return html\n",
    "\n",
    "def extract_first_table(html):\n",
    "    match = re.search(r'(<table width=\"600\" class=\"SAPBEXBorderlessFlexBox\" cellspacing=\"0\" cellpadding=\"0\" border=\"2\">.*?</table>)', html, flags=re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(1) if match else \"Table not found\"\n",
    "\n",
    "def table_to_dataframe(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find the table that contains the data\n",
    "    data_table = soup.find(\"table\", class_=\"SAPBEXTableGrid\")\n",
    "    # If no table is found, return an empty DataFrame with expected columns\n",
    "    if not data_table:\n",
    "        print(\"Table with class 'SAPBEXTableGrid' not found.\")\n",
    "        return pd.DataFrame(columns=[\"Object Type\", \"Name\", \"Technical Name\", \"Source System\", \"HANA CV Name\"])\n",
    "\n",
    "    rows = data_table.find_all(\"tr\")\n",
    "    data = []\n",
    "    current_object_type = \"\"\n",
    "\n",
    "    # Iterate over table rows, skipping the header row\n",
    "    for row in rows[1:]:  # Skip the header\n",
    "        cells = row.find_all(\"td\")\n",
    "        # Check if we have 3 columns in this row\n",
    "        if len(cells) == 3:\n",
    "            object_type = cells[0].text.strip() or current_object_type\n",
    "            name = cells[1].text.strip()  # Keep the original name unchanged\n",
    "            technical_name = cells[2].text.strip()\n",
    "            # Initialize additional columns\n",
    "            source_system = \"\"\n",
    "            hana_cv_name = \"\"\n",
    "\n",
    "            # Check if object type is Data Source (case-insensitive) and technical name contains \"HANA\"\n",
    "            if \"data\" in object_type.lower() and \"source\" in object_type.lower() and \"hana\" in technical_name.lower():\n",
    "                # Collapse multiple spaces inside the technical name to a single space\n",
    "                technical_name = re.sub(r'\\s+', ' ', technical_name).strip()  # Collapse spaces\n",
    "                # Split the technical name by the first space\n",
    "                parts = technical_name.split(\" \", 1)\n",
    "                # Limit Technical Name to the first part (before the first space)\n",
    "                technical_name = parts[0]\n",
    "                # Extract Source System (everything after the first space)\n",
    "                if len(parts) > 1:\n",
    "                    source_system = parts[1].strip()  # Everything after the first part\n",
    "                # Extract Hana Calculation View Name from the transformed version of the 'Name'\n",
    "                transformed_name = re.sub(r'[/:->]', ' ', name)  # Replace the delimiters with spaces\n",
    "                hana_cv_name = transformed_name.split()[-1]  # Take the last part of the transformed string\n",
    "            # Append the row with the new columns\n",
    "            data.append([object_type, name, technical_name, source_system, hana_cv_name])\n",
    "            if cells[0].text.strip():\n",
    "                current_object_type = cells[0].text.strip()\n",
    "\n",
    "    # Create a pandas DataFrame with additional columns\n",
    "    df = pd.DataFrame(data, columns=[\"Object Type\", \"Name\", \"Technical Name\", \"Source System\", \"HANA CV Name\"])\n",
    "    return df\n",
    "\n",
    "def extract_transformation_names(df):\n",
    "    \"\"\"\n",
    "    Extracts values from the 'Technical Name' column for rows where 'Object Type' contains 'Transformation'.\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame.\n",
    "    Returns:\n",
    "    list: A list of extracted 'Technical Name' values.\n",
    "    \"\"\"\n",
    "    return df.loc[df['Object Type'].str.contains('Transformation', na=False, case=False), 'Technical Name'].tolist()\n",
    "\n",
    "def extract_tables_from_html(html, transformation_names):\n",
    "    \"\"\"\n",
    "    Extracts parent <table> elements containing <a name=\"TRFN{item}\"> from an HTML file.\n",
    "    Parameters:\n",
    "    html (str): The HTML file content.\n",
    "    transformation_names (list): List of transformation names.\n",
    "    Returns:\n",
    "    list: A list of extracted table HTML content as strings.\n",
    "    \"\"\"\n",
    "    extracted_tables = []\n",
    "    # Read the HTML file\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Iterate through transformation names\n",
    "    for name in transformation_names:\n",
    "        anchor_tag = soup.find(\"a\", {\"name\": f\"TRFN{name}\"})  # Find the first matching <a> tag\n",
    "        if anchor_tag:\n",
    "            table_tag = anchor_tag.find_parent(\"table\", {\n",
    "                \"width\": \"600\",\n",
    "                \"class\": \"SAPBEXBorderlessFlexBox\",\n",
    "                \"cellspacing\": \"0\",\n",
    "                \"cellpadding\": \"0\",\n",
    "                \"border\": \"1\"\n",
    "            })\n",
    "            if table_tag:\n",
    "                extracted_tables.append(str(table_tag))  # Store as HTML string\n",
    "    return extracted_tables\n",
    "\n",
    "def extract_transf_details(tables_list, file_names, output_dir):\n",
    "    \"\"\"\n",
    "    Extracts SAP transformation details from HTML tables using Azure OpenAI and saves the details to text files.\n",
    "    Parameters:\n",
    "    tables_list (list): List of HTML table strings.\n",
    "    file_names (list): List of filenames for storing transformation details.\n",
    "    output_dir (str): Directory where transformation details will be saved.\n",
    "    Returns:\n",
    "    list: List of model-generated transformation/table details.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    routines = []\n",
    "    for index, table_html in enumerate(tables_list):\n",
    "        dfs = pd.read_html(table_html)\n",
    "        table_html_df = dfs[0].dropna(how='all')\n",
    "        if len(table_html_df.columns) == 2:\n",
    "            table_html_df.columns = ['Logic', 'Field_name']   \n",
    "            table_processed_df = table_html_df\n",
    "\n",
    "            table_description_html = table_processed_df.iloc[-1]['Logic']\n",
    "\n",
    "            start_index = table_description_html.find(\"Routine \") + len(\"Routine \")\n",
    "            end_index = table_description_html.find(\" Required Objects Object Type Name Technical Name Routine\")\n",
    "            routine_value = table_description_html[start_index:end_index] \n",
    "\n",
    "            start_index = table_description_html.find(\"Technical Name: \") + len(\"Technical Name: \")\n",
    "            end_index = table_description_html.find(\" Description (Short): \")\n",
    "            technical_name_value = table_description_html[start_index:end_index]\n",
    "\n",
    "            start_index = table_description_html.find(\"Description (Short): \") + len(\"Description (Short): \")\n",
    "            end_index = table_description_html.find(\" Description (Long): \")\n",
    "            short_desciption_value = table_description_html[start_index:end_index]\n",
    "\n",
    "            start_index = table_description_html.find(\"Description (Long): \") + len(\"Description (Long): \")\n",
    "            end_index = table_description_html.find(\" Object Version: \")\n",
    "            long_desciption_value = table_description_html[start_index:end_index]\n",
    "\n",
    "            start_index = table_description_html.find(\"Source: \") + len(\"Source: \")\n",
    "            end_index = table_description_html.find(\" Target: \")\n",
    "            source_value = table_description_html[start_index:end_index]\n",
    "\n",
    "            start_index = table_description_html.find(\"Target: \") + len(\"Target: \")\n",
    "            target_value = table_description_html[start_index:start_index+20]\n",
    "\n",
    "            transformation_info = pd.DataFrame({\n",
    "                'Technical_name': [technical_name_value],\n",
    "                'Short_description': [short_desciption_value],\n",
    "                'Long_description': [long_desciption_value],\n",
    "                'Source': [source_value],\n",
    "                'Target': [target_value],\n",
    "                'Routine': [routine_value]\n",
    "            })\n",
    "\n",
    "            #display(transformation_info)\n",
    "            \n",
    "        if len(table_html_df.columns) == 11:\n",
    "            table_html_df.columns = ['Logic', 'Field_name', 'Description', 'Data_Type', 'Length', 'Rule_Type', 'Target', 'InfoObject', 'Description1', 'Data_Type1', 'Length1']\n",
    "            \n",
    "            table_html_df = table_html_df[table_html_df['Field_name'].str[:9] != \"Unit Info\"]\n",
    "            table_html_df = table_html_df[(table_html_df['Field_name'].notnull()) | (table_html_df['InfoObject'].notnull())].reset_index(drop=True)\n",
    "\n",
    "            same_value_rows = {index: row['Logic'] for index, row in table_html_df.iterrows() if len(set(row)) == 1 and index not in [table_html_df.index.max(), table_html_df.index.max() - 1]}\n",
    "            rule_rows = {index: row['Rule_Type'] for index, row in table_html_df.iterrows()}\n",
    "\n",
    "            for i in range(len(same_value_rows) -3):\n",
    "                if list(same_value_rows.keys())[i+1] - list(same_value_rows.keys())[i] == 1 and 'Constant' in str(list(same_value_rows.values())[i+1]):\n",
    "                    same_value_rows[list(same_value_rows.keys())[i]] = same_value_rows[list(same_value_rows.keys())[i]] + '\\n' + same_value_rows[list(same_value_rows.keys())[i+1]]\n",
    "                    del same_value_rows[list(same_value_rows.keys())[i+1]]\n",
    "                \n",
    "            table_processed_df = table_html_df\n",
    "\n",
    "            for key, value in same_value_rows.items():\n",
    "                index = key - 1\n",
    "                if not(pd.isna(table_processed_df.iloc[index]['Rule_Type'])):\n",
    "                    table_processed_df.at[index, 'Logic'] = str(value)\n",
    "                else:\n",
    "                    index -= 1\n",
    "                    if not(pd.isna(table_processed_df.iloc[index]['Rule_Type'])):\n",
    "                        table_processed_df.at[index, 'Logic'] = str(value)\n",
    "\n",
    "            for index, row in table_processed_df.iterrows():\n",
    "                if len(set(row)) == 1 and index not in [table_html_df.index.max(), table_html_df.index.max() - 1]:\n",
    "                    table_processed_df = table_processed_df.drop(index)\n",
    "\n",
    "            table_description_html = table_processed_df.iloc[-1]['Logic']\n",
    "\n",
    "            start_index = table_description_html.find(\"Technical Name: \") + len(\"Technical Name: \")\n",
    "            end_index = table_description_html.find(\" Description (Short): \")\n",
    "            technical_name_value = table_description_html[start_index:end_index]\n",
    "\n",
    "            start_index = table_description_html.find(\"Description (Short): \") + len(\"Description (Short): \")\n",
    "            end_index = table_description_html.find(\" Description (Long): \")\n",
    "            short_desciption_value = table_description_html[start_index:end_index]\n",
    "\n",
    "            start_index = table_description_html.find(\"Description (Long): \") + len(\"Description (Long): \")\n",
    "            end_index = table_description_html.find(\" Object Version: \")\n",
    "            long_desciption_value = table_description_html[start_index:end_index]\n",
    "\n",
    "            start_index = table_description_html.find(\"Source: \") + len(\"Source: \")\n",
    "            end_index = table_description_html.find(\" Target: \")\n",
    "            source_value = table_description_html[start_index:end_index]\n",
    "\n",
    "            start_index = table_description_html.find(\"Target: \") + len(\"Target: \")\n",
    "            target_value = table_description_html[start_index:start_index+20]\n",
    "            if 'Routine' in table_description_html:\n",
    "                start_index = table_description_html.find(\"Code for Start Routine \") + len(\"Code for Start Routine \")\n",
    "                if \"Inverse Startroutine Routine Code for Inverse Start Routine\" in table_description_html:\n",
    "                    start_index = table_description_html.find(\"Inverse Startroutine Routine Code for Inverse Start Routine \") + len(\"Inverse Startroutine Routine Code for Inverse Start Routine \")\n",
    "                if \"Routine Code for End Routine\" in table_description_html:\n",
    "                    start_index = table_description_html.find(\"Routine Code for End Routine \") + len(\"Routine Code for End Routine \")\n",
    "                end_index = table_description_html.find(\" Required Objects Object Type Name Technical Name\")\n",
    "                if \"Source Field name Description Data Type Length Rule Type Target InfoObject Description Data Type Length\" in table_description_html:\n",
    "                    end_index = table_description_html.find(\" Source Field name Description Data Type Length Rule Type Target InfoObject Description Data Type Length\")\n",
    "                if \"Source InfoObject Description Data Type Length Rule Type Target InfoObject Description Data Type Length\" in table_description_html:\n",
    "                    end_index = table_description_html.find(\" Source InfoObject Description Data Type Length Rule Type Target InfoObject Description Data Type Length\")\n",
    "                routine_value = table_description_html[start_index:end_index]\n",
    "            else:\n",
    "                routine_value = None\n",
    "\n",
    "            table_processed_df = table_processed_df.drop(table_processed_df.tail(2).index)\n",
    "            table_processed_df = table_processed_df.drop(columns=['Target'])\n",
    "            table_processed_df.insert(0, 'Target', target_value)\n",
    "            table_processed_df.insert(0, 'Source', source_value)\n",
    "            table_processed_df.insert(0, 'Long_description', long_desciption_value)\n",
    "            table_processed_df.insert(0, 'Short_description', short_desciption_value)\n",
    "            table_processed_df.insert(0, 'Technical_name', technical_name_value)\n",
    "            table_processed_df.loc[table_processed_df['Logic'].notnull(), 'Routine'] = routine_value\n",
    "            table_processed_df = table_processed_df[['Technical_name','Short_description','Long_description','Source','Target','Field_name','Logic','Description','Data_Type','Length','Rule_Type','InfoObject','Description1','Data_Type1', 'Length1', 'Routine']]\n",
    "            #display(table_processed_df.assign(result=table_processed_df.to_dict(orient='records')).loc[table_processed_df['Routine'].notnull().idxmax(), ['Technical_name', 'Routine', 'result']].to_frame().T)\n",
    "            display(table_processed_df)\n",
    "\n",
    "        # extraction_prompt = f\"\"\"You are given an HTML structure below describing a SAP transformation.\n",
    "        # Focus on the Key Rules and/or the Data Rules. Extract the required fields in csv format with\n",
    "        # the following header: Source Name, Target Name, Source Field Name, Source Field Description,\n",
    "        # Source Field Data Type, Source Field Length, Rule Type, Target InfoObject Name, Target InfoObject Description,\n",
    "        # Target InfoObject Data Type, Target InfoObject Length. \\n\\nHTML structure:\\n{table_html}\"\"\"\n",
    "        # abap_prompt = f\"\"\"You are given an HTML structure below describing a SAP transformation.\n",
    "        # Focus on the ABAP code present in the structure. Extract the required code and store in JSON format\n",
    "        # with the following keys: Start Routine, Global Code, Global Code 2, End Routine, Invers Endroutine.\n",
    "        # Ignore any statements that say '... \"insert your code here', do not include them in the extract.\n",
    "        # If you cannot find code for one key simply leave it blank. \\n\\nHTML structure:\\n{table_html}\"\"\"\n",
    "        # try:\n",
    "        #     # Call Azure OpenAI model for chat completion\n",
    "        #     details_response = call_model(selected_model, prompt=extraction_prompt)\n",
    "        #     #details_response = call_model_o1(deployment_name=\"o1\", prompt=extraction_prompt)\n",
    "        #     summaries.append(details_response)\n",
    "        #     abap_response = call_model(selected_model, prompt=abap_prompt)\n",
    "        #     #abap_response = call_model_o1(deployment_name=\"o1\", prompt=abap_prompt)\n",
    "        #     routines.append(abap_response)\n",
    "        #     # Write the summary to a text file\n",
    "        #     file_name = f\"{file_names[index]}.txt\"\n",
    "        #     file_path = f\"{output_dir}/{file_name}\"\n",
    "        #     dbutils.fs.put(file_path, f\"Transformation details:\\n{details_response}\\n\\nTransformation ABAP code:\\n{abap_response}\\n\", overwrite=True)\n",
    "        # except Exception as e:\n",
    "        #     log_message = f\"Error processing table {index}: {e}\"\n",
    "        #     log_messages.append(f\"**Error**: {log_message}\\n\")\n",
    "        #     #print(log_message)\n",
    "    return summaries, routines\n",
    "\n",
    "def convert_transf(tables_details, routine_codes, file_names, output_dir):\n",
    "    \"\"\"\n",
    "    Translate SAP transformation details from detail tables using Azure OpenAI and saves the code to text files.\n",
    "    Parameters:\n",
    "    tables_details (list): List of table transformation details.\n",
    "    routine_codes (list): List of transformation ABAP code.\n",
    "    file_names (list): List of filenames for storing transformation details.\n",
    "    output_dir (str): Directory where transformation details will be saved.\n",
    "    Returns:\n",
    "    list: List of model-generated code that replicates transformation behaviour on Databricks.\n",
    "    \"\"\"\n",
    "    conversions = []\n",
    "    for index, table_html in enumerate(tables_details):\n",
    "        conversion_prompt = f\"\"\"Below you are given two major pieces of a SAP transformation, the transformation\n",
    "        details with source and target fields and the routine code used in the transformation. Using these details\n",
    "        do the following:\n",
    "        1. Write SQL code that runs on Databricks that builds the source table structure.\n",
    "        2. Write SQL code that runs on Databricks that builds the target table structure.\n",
    "        3. Write SQL code that runs on Databricks that writes data from source table to target table in the following order:\n",
    "            i. 1:1 mappings (i.e. \"[DIRECT]\")\n",
    "            ii. \"[CONSTANT]\" fields\n",
    "            iii. Transformation rules/logic in the routine code\n",
    "            iv. Any other logic deduced elsewhere (mention it explicitly in the output)\n",
    "        Optimize the code for Databricks. Assume that the environment in Databricks has already been created,\n",
    "        thus adhere strictly to generating the code for the two tables without any setup steps. Write comments\n",
    "        explaining code logic. \\n\\nTRANSFORMATION DETAILS:\\n{table_html} \\n\\nROUTINE CODE:\\n{routine_codes[index]}\"\"\"\n",
    "        try:\n",
    "            # Call Azure OpenAI model for chat completion\n",
    "            #response = call_model_o1(deployment_name=\"o1\", prompt=conversion_prompt)\n",
    "            response = call_model(selected_model, prompt=conversion_prompt)\n",
    "            conversions.append(response)\n",
    "            # Write the summary to a text file\n",
    "            file_name = f\"{file_names[index]}.txt\"\n",
    "            file_path = f\"{output_dir}/{file_name}\"\n",
    "            dbutils.fs.put(file_path, f\"Transformation code:\\n{response}\", overwrite=True)\n",
    "        except Exception as e:\n",
    "            log_message = f\"Error processing table {index}: {e}\"\n",
    "            log_messages.append(f\"**Error**: {log_message}\\n\")\n",
    "            #print(log_message)\n",
    "    return conversions\n",
    "\n",
    "\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "input_directory_path = f\"{input_directory_base_path}/\"\n",
    "\n",
    "# List files in the source directory for the current dataflow\n",
    "files = [f for f in os.listdir(input_directory_path)]\n",
    "\n",
    "# Keep only BW source files for processing\n",
    "bw_files_to_process = [file for file in files]\n",
    "\n",
    "# Initialize a list to store all log messages for the current DataFlow\n",
    "log_messages = []\n",
    "\n",
    "# Process each file in the source directory\n",
    "for file_info in bw_files_to_process:\n",
    "    file_path = HTML_FILE\n",
    "    processed_html = process_html_spark(file_path)\n",
    "    extracted_table = extract_first_table(processed_html)\n",
    "    df = table_to_dataframe(extracted_table)\n",
    "    #print('First table: ')\n",
    "    #display(df)\n",
    "    transformations_technical_names = extract_transformation_names(df)\n",
    "    #print(f'transformations_technical_names = {transformations_technical_names}')\n",
    "    transformations_technical_names = [filtered_transf for filtered_transf in transformations_technical_names]# if filtered_transf in list_of_filtered_trans]\n",
    "    #print(transformations_technical_names)\n",
    "    transformation_tables = extract_tables_from_html(processed_html, transformations_technical_names)\n",
    "    transformation_details, transformation_routines = extract_transf_details(transformation_tables, transformations_technical_names, '/transformation_detail')\n",
    "    #print(f'transformation_details = {transformation_details}')\n",
    "    #print(f'transformation_routines = {transformation_routines}')\n",
    "    transformation_code = convert_transf(transformation_details, transformation_routines, transformations_technical_names, '/transformation_detail')\n",
    "    #print(f'transformation_code = {transformation_code}')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4962541885738502,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bw_html_to_sql_utility",
   "widgets": {
    "DataFlow": {
     "currentValue": "1",
     "nuid": "f80c81e7-a203-4f42-8c6b-13e2abb852d4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "DataFlow",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "DataFlow",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "model_choice": {
     "currentValue": "o1",
     "nuid": "6b3f1385-3f91-4351-825a-a3fa142c9e07",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "o1",
      "label": "Choose model type",
      "name": "model_choice",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "o1",
        "o1-mini",
        "o1-preview"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "o1",
      "label": "Choose model type",
      "name": "model_choice",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "o1",
        "o1-mini",
        "o1-preview"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
