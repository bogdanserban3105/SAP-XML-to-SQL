{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd5002f-4a5c-449b-b2f0-1de4ad2941a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781ac1dc-be41-4bec-a39a-263dd1133702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Create a text input widget to select the folder\n",
    "dbutils.widgets.text(\"insert_data_flow_folder\", \"\", \"Enter the name of the DataFlow source folder to process (e.g., DF1, DF2..DFn, etc)\")\n",
    "\n",
    "# Create a dropdown widget for selecting model type (o1 or o1 mini)\n",
    "dbutils.widgets.dropdown(\"model_choice\", \"o1\", [\"o1\", \"o1-mini\"], \"Choose model type\")\n",
    "\n",
    "# Retrieve the selected folder and model from the widgets\n",
    "selected_folder = dbutils.widgets.get(\"insert_data_flow_folder\")\n",
    "selected_folder = selected_folder.upper()\n",
    "\n",
    "if not selected_folder:\n",
    "    raise ValueError(\"Please provide a folder name in the widget.\")\n",
    "\n",
    "# Retrieve the selected model type\n",
    "selected_model = dbutils.widgets.get(\"model_choice\")\n",
    "\n",
    "# # Set your storage account SAS token and name\n",
    "# sasToken = \"placeholder\"  # Replace with your actual SAS token\n",
    "# sa = \"erdccalearning\"  # Replace with your actual storage account name\n",
    "# container_name = \"nndemo\"  # Replace with your actual container name\n",
    "# spark.conf.set(f\"fs.azure.account.auth.type.{sa}.dfs.core.windows.net\", \"SAS\")\n",
    "# spark.conf.set(f\"fs.azure.sas.token.provider.type.{sa}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "# spark.conf.set(f\"fs.azure.sas.nndemo.{sa}.blob.core.windows.net\", f\"{sasToken}\")\n",
    "\n",
    "# # Set the base URL for the container and directories\n",
    "# base_url = f\"wasbs://{container_name}@{sa}.blob.core.windows.net/AcceleratorSAPFiles\"\n",
    "\n",
    "sa = \"stlpdel01dev\"\n",
    "container_name = \"codeconverter\" # Replace with your actual container name\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"CustomAccessToken\")\n",
    "spark.conf.set(\"fs.azure.account.custom.token.provider.class\", spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\"))\n",
    "\n",
    "base_path = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/\"\n",
    "\n",
    "validated_files_directory = \"ValidatedFiles\"\n",
    "cleaned_files_directory = \"CleansedSQLFiles\"\n",
    "archived_files_directory = \"ArchivedValidatedFiles\"\n",
    "\n",
    "\n",
    "# Function to list all files in a directory recursively\n",
    "def list_all_files(directory_path):\n",
    "    files = []\n",
    "    for file_info in dbutils.fs.ls(directory_path):\n",
    "        if file_info.isFile():\n",
    "            files.append(file_info.path)\n",
    "        else:\n",
    "            files.extend(list_all_files(file_info.path))\n",
    "    return files\n",
    "\n",
    "# Function to perform double cleaning operation on SQL output\n",
    "def cleanse_sql_output(output: str) -> str:\n",
    "    cleaned_output = re.sub(r'```sql', '', output)\n",
    "    cleaned_output = re.sub(r'`+', '', cleaned_output)\n",
    "    return cleaned_output\n",
    "\n",
    "def read_file_content(file_path):\n",
    "    \"\"\"Read the content of a file using Spark's higher-level API (spark.read.text).\"\"\"\n",
    "    try:\n",
    "        df = spark.read.text(file_path)\n",
    "        file_content = df.rdd.map(lambda r: r[0]).collect()\n",
    "        return file_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {file_path}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to call the appropriate model based on selected type\n",
    "def call_model(selected_model, prompt):\n",
    "    if selected_model == \"o1\":\n",
    "        return call_model_o1(deployment_name=\"o1\", prompt=prompt)  # Call the o1 model function\n",
    "    elif selected_model == \"o1-mini\":\n",
    "        return callmodelo1(deployment_name=\"o1-mini\", prompt=prompt)  # Call the o1-mini model function\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model choice\")\n",
    "\n",
    "\n",
    "# Helper function to join paths correctly (handling double slashes)\n",
    "def join_paths(base, *paths):\n",
    "    # Strip leading slash from subsequent paths to avoid double slashes\n",
    "    return os.path.join(base, *[path.lstrip(\"/\") for path in paths])\n",
    "\n",
    "\n",
    "# Function to process selected folder\n",
    "def process_folder(source_directory):\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Update the path to correctly point to the selected subfolder in the ValidatedFiles directory\n",
    "    source_directory_path = join_paths(base_path, validated_files_directory, source_directory)\n",
    "    cleaned_directory_path = join_paths(base_path, cleaned_files_directory, source_directory, current_date)\n",
    "    archive_directory_path = join_paths(base_path, archived_files_directory, source_directory, current_date)\n",
    "\n",
    "    all_files = list_all_files(source_directory_path)\n",
    "\n",
    "    for file_path in all_files:\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        try:\n",
    "            file_content = read_file_content(file_path)\n",
    "            if file_content is None:\n",
    "                continue\n",
    "            \n",
    "            file_content_as_string = \"\\n\".join(file_content)\n",
    "            prompt = f\"\"\"\n",
    "            Clean the following SQL query by removing comments and unwanted characters, making it ready for execution without changing its logic. Remove all backticks (`) and only retain the final version of the SQL query. After cleaning, optimize the query for performance, ensuring to avoid using Common Table Expressions (CTEs) wherever possible. If the query contains an inner query (subquery), please replace the inner query with a Common Table Expression (CTE) for improved readability and structure.\n",
    "            \n",
    "            When writing SQL queries, please make sure to explicitly define each column you are selecting. Avoid using SELECT *, and instead, list out the specific columns needed. This will help in optimizing performance, making the query more readable, and ensuring clarity in the data being fetched.\n",
    "            \n",
    "            \n",
    "            SQL Code: \\n{file_content_as_string}\n",
    "            \"\"\"\n",
    "\n",
    "            model_output = call_model(selected_model, prompt)\n",
    "            \n",
    "            reassess_prompt = f\"\"\"\n",
    "\n",
    "                \"Please confirm that the following SQL code has been properly cleaned and optimized. The optimization should focus on improving performance, specifically avoiding the use of CTEs (Common Table Expressions) if feasible. After cleaning and optimization, ensure that:\n",
    "\n",
    "                All backticks (`) are removed from the text.\n",
    "\n",
    "\n",
    "                The code is optimized for performance, considering any opportunities to remove CTEs and refactor the query for better efficiency. If the query contains an inner query (subquery), please replace the inner query with a Common Table Expression (CTE) for improved readability and structure.\n",
    "\n",
    "                Only the latest version of the SQL query is returned, with all unnecessary complexity reduced.\n",
    "\n",
    "                Please make sure that the final output is a clean, optimized SQL query ready for automatic execution by the engine.\n",
    "\n",
    "                When writing SQL queries, please make sure to explicitly define each column you are selecting. Avoid using SELECT *, and instead, list out the specific columns needed. This will help in optimizing performance, making the query more readable, and ensuring clarity in the data being fetched.\n",
    "\n",
    "                Original SQL code: {file_content_as_string}\n",
    "\n",
    "                Cleaned and optimized SQL code: {model_output}\"\n",
    "\n",
    "            \"\"\"\n",
    "            reassessed_output = call_model(selected_model, reassess_prompt)\n",
    "            \n",
    "            reassessed_output = cleanse_sql_output(reassessed_output)\n",
    "            \n",
    "            # Ensure the subfolder structure is preserved when saving the cleaned file\n",
    "            relative_path = file_path.replace(source_directory_path, '').strip('/')\n",
    "\n",
    "            # Construct the output file path\n",
    "            output_file_path = join_paths(cleaned_directory_path, relative_path)\n",
    "\n",
    "            # Ensure that the output directory exists\n",
    "            cleaned_subdirectory = os.path.dirname(output_file_path)\n",
    "\n",
    "            # Ensure the cleaned subdirectory exists\n",
    "            if not dbutils.fs.mkdirs(cleaned_subdirectory):\n",
    "                print(f\"Directory {cleaned_subdirectory} already exists or created successfully.\")\n",
    "            \n",
    "            # Save the cleaned file\n",
    "            dbutils.fs.put(output_file_path, reassessed_output, overwrite=True)\n",
    "            print(f\"Successfully processed and saved cleaned SQL code for file: {file_path}\")\n",
    "            \n",
    "            # Archive the original file in the correct location\n",
    "            archive_file_path = join_paths(archive_directory_path, relative_path)\n",
    "            archive_date_folder = os.path.dirname(archive_file_path)\n",
    "\n",
    "            # Ensure the archive directory exists\n",
    "            if not dbutils.fs.mkdirs(archive_date_folder):\n",
    "                print(f\"Directory {archive_date_folder} already exists or created successfully.\")\n",
    "            \n",
    "            final_archive_path = join_paths(archive_date_folder, os.path.basename(file_path))\n",
    "\n",
    "            dbutils.fs.mv(file_path, final_archive_path)\n",
    "            print(f\"Successfully archived the original validated file to: {final_archive_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "\n",
    "# Process the selected folder\n",
    "process_folder(selected_folder)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clean_sql_queries_v2",
   "widgets": {
    "insert_data_flow_folder": {
     "currentValue": "df34",
     "nuid": "86160374-ec93-4127-a045-3802d06bca7e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter the name of the DataFlow source folder to process (e.g., DF1, DF2..DFn, etc)",
      "name": "insert_data_flow_folder",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "Enter the name of the DataFlow source folder to process (e.g., DF1, DF2..DFn, etc)",
      "name": "insert_data_flow_folder",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "model_choice": {
     "currentValue": "o1",
     "nuid": "8d9de721-c099-48f7-a7ae-8d8f87d2b200",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "o1",
      "label": "Choose model type",
      "name": "model_choice",
      "options": {
       "choices": [
        "o1",
        "o1-mini"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "o1",
      "label": "Choose model type",
      "name": "model_choice",
      "options": {
       "autoCreated": null,
       "choices": [
        "o1",
        "o1-mini"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
