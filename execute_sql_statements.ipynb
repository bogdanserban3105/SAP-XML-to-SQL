{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629b3e1d-cdcc-49e4-ad7b-35d960e57fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1762f745-86bb-4303-8859-db21e5f329e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# # Set your storage account SAS token and name\n",
    "# sasToken = \"placeholder\" # Replace with your actual SAS token\n",
    "# sa = \"erdccalearning\" # Replace with your actual storage account name\n",
    "# container_name = \"nndemo\" # Replace with your actual container name\n",
    "\n",
    "# # Set the base URL for the container and directories\n",
    "# base_url = f\"wasbs://{container_name}@{sa}.blob.core.windows.net/AcceleratorSAPFiles/\"\n",
    "# spark.conf.set(f\"fs.azure.account.auth.type.{sa}.dfs.core.windows.net\", \"SAS\")\n",
    "# spark.conf.set(f\"fs.azure.sas.token.provider.type.{sa}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "# spark.conf.set(f\"fs.azure.sas.nndemo.{sa}.blob.core.windows.net\", f\"{sasToken}\")\n",
    "\n",
    "sa = \"stlpdel01dev\"\n",
    "container_name = \"codeconverter\" # Replace with your actual container name\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"CustomAccessToken\")\n",
    "spark.conf.set(\"fs.azure.account.custom.token.provider.class\", spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\"))\n",
    "\n",
    "base_path = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/\"\n",
    "\n",
    "cleaned_directory_path = f\"{base_url}CleanedSQLFiles/\"\n",
    "archive_exec_directory_path = f\"{base_url}ArchivedExecutedSQLFiles/\"\n",
    "\n",
    "# Create an archive folder with the current date\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "archive_exec_date_folder = f\"{archive_exec_directory_path.rstrip('/')}/{current_date}/\"\n",
    "dbutils.fs.mkdirs(archive_exec_date_folder)\n",
    "\n",
    "# List files in the cleansed directory\n",
    "files = dbutils.fs.ls(cleaned_directory_path)\n",
    "\n",
    "# Process each file in the cleaned directory\n",
    "for file_info in files:\n",
    "    file_path = file_info.path\n",
    "    if file_info.isFile():\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        try:\n",
    "            # Read file content\n",
    "            file_content = read_file_content(file_path)\n",
    "            if file_content is None:\n",
    "                continue\n",
    "            \n",
    "            # Convert file content to a single string\n",
    "            sql_code = \"\\n\".join(file_content)\n",
    "            \n",
    "            # Split the SQL code into individual statements\n",
    "            statements = sql_code.split(\";\")\n",
    "            \n",
    "            for statement in statements:\n",
    "                statement = statement.strip()\n",
    "                if statement:\n",
    "                    # Execute each SQL statement individually\n",
    "                    print(f\"Executing SQL statement: {statement}\")\n",
    "                    spark.sql(statement)\n",
    "            \n",
    "            print(f\"Successfully executed SQL code for file: {file_path}\")\n",
    "            \n",
    "            # Archive the executed file to the archive directory\n",
    "            archive_file_path = f\"{archive_exec_date_folder}{os.path.basename(file_path)}\"\n",
    "            dbutils.fs.mv(file_path, archive_file_path)\n",
    "            print(f\"Successfully archived the executed file to: {archive_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping directory: {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "execute_sql_statements",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
