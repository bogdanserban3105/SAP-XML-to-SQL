{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe926c16-9c46-4f46-a2b9-c2a0e978ce87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fdc265a-2beb-401f-a396-43437b72d4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import AzureOpenAI\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1847786-e99d-4ebf-a6b3-9c7f7a30d786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a dropdown widget for selecting model type (o1 or o1 mini)\n",
    "dbutils.widgets.dropdown(\"model_choice\", \"o1\", [\"o1\", \"o1-mini\", \"o1-preview\"], \"Choose model type\")\n",
    "# Retrieve the selected model type\n",
    "selected_model = dbutils.widgets.get(\"model_choice\")\n",
    "print(f\"Selected model: {selected_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8216c781-4d82-4c82-be15-660623cf7e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to call the appropriate model based on selected type\n",
    "def call_model(selected_model, prompt):\n",
    "    if selected_model == \"o1\":\n",
    "        return call_model_o1(deployment_name=\"o1\", prompt=prompt)  # Call the o1 model function\n",
    "    elif selected_model == \"o1-preview\":\n",
    "        return callmodelo1(deployment_name=\"o1-preview\", prompt=prompt)  # Call the o1-preview model function\n",
    "    elif selected_model == \"o1-mini\":\n",
    "        return callmodelo1(deployment_name=\"o1-mini\", prompt=prompt)  # Call the o1-mini model function\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model choice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bf70c5-3299-4b68-a2fd-4a110b964976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Set your storage account SAS token and name\n",
    "# sasToken = dbutils.secrets.get(scope=\"codeconv\", key=\"sasTokenKey\")  # Create your own Scope in DataBricks and assign a secret for it\n",
    "# sa = \"erdccalearning\"  # Replace with your actual storage account name\n",
    "# container_name = \"nndemo\"  # Replace with your actual container name\n",
    "# spark.conf.set(f\"fs.azure.account.auth.type.{sa}.dfs.core.windows.net\", \"SAS\")\n",
    "# spark.conf.set(f\"fs.azure.sas.token.provider.type.{sa}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "# spark.conf.set(f\"fs.azure.sas.nndemo.{sa}.blob.core.windows.net\", f\"{sasToken}\")\n",
    "\n",
    "# # Set the base URL for the container and directories\n",
    "# base_url = f\"wasbs://{container_name}@{sa}.blob.core.windows.net/AcceleratorSAPFiles/\"\n",
    "\n",
    "sa = \"stlpdel01dev\"\n",
    "container_name = \"codeconverter\" # Replace with your actual container name\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"CustomAccessToken\")\n",
    "spark.conf.set(\"fs.azure.account.custom.token.provider.class\", spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\"))\n",
    "base_url = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/\"\n",
    "\n",
    "# Define paths\n",
    "validated_directory_base_path = f\"{base_url}ValidatedFiles/\"\n",
    "archived_directory_base_path = f\"{base_url}ArchivedFiles/\"\n",
    "logs_directory_base_path = f\"{base_url}Logs/\"\n",
    "input_directory_base_path = f\"{base_url}InputFiles/\"\n",
    "\n",
    "# Ensure base directories exist\n",
    "dbutils.fs.mkdirs(validated_directory_base_path)\n",
    "dbutils.fs.mkdirs(archived_directory_base_path)\n",
    "dbutils.fs.mkdirs(logs_directory_base_path)\n",
    "\n",
    "# Create a widget to capture DataFlow input from the user\n",
    "dbutils.widgets.text(\"DataFlow\", \"\")\n",
    "dataflow_input = dbutils.widgets.get(\"DataFlow\")\n",
    "\n",
    "# Ensure the DataFlow input is uppercase\n",
    "dataflow = dataflow_input.upper()\n",
    "\n",
    "# Define functions\n",
    "def process_html_spark(file_path):\n",
    "    # Read the HTML file as plain text into a DataFrame\n",
    "    df = spark.read.text(file_path)\n",
    "    # Combine all lines of the HTML file into one string\n",
    "    html = \"\\n\".join([row['value'] for row in df.collect()])\n",
    "    # Remove everything inside <HEAD>...</HEAD>\n",
    "    html = re.sub(r'<HEAD>.*?</HEAD>', '', html, flags=re.DOTALL | re.IGNORECASE)\n",
    "    # Remove all <img ...> tags\n",
    "    html = re.sub(r'<img([\\w\\W]+?)>', '', html, flags=re.IGNORECASE)\n",
    "    return html\n",
    "\n",
    "def extract_first_table(html):\n",
    "    match = re.search(r'(<table width=\"600\" class=\"SAPBEXBorderlessFlexBox\" cellspacing=\"0\" cellpadding=\"0\" border=\"2\">.*?</table>)', html, flags=re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(1) if match else \"Table not found\"\n",
    "\n",
    "def table_to_dataframe(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find the table that contains the data\n",
    "    data_table = soup.find(\"table\", class_=\"SAPBEXTableGrid\")\n",
    "    # If no table is found, return an empty DataFrame with expected columns\n",
    "    if not data_table:\n",
    "        print(\"Table with class 'SAPBEXTableGrid' not found.\")\n",
    "        return pd.DataFrame(columns=[\"Object Type\", \"Name\", \"Technical Name\", \"Source System\", \"HANA CV Name\"])\n",
    "\n",
    "    rows = data_table.find_all(\"tr\")\n",
    "    data = []\n",
    "    current_object_type = \"\"\n",
    "\n",
    "    # Iterate over table rows, skipping the header row\n",
    "    for row in rows[1:]:  # Skip the header\n",
    "        cells = row.find_all(\"td\")\n",
    "        # Check if we have 3 columns in this row\n",
    "        if len(cells) == 3:\n",
    "            object_type = cells[0].text.strip() or current_object_type\n",
    "            name = cells[1].text.strip()  # Keep the original name unchanged\n",
    "            technical_name = cells[2].text.strip()\n",
    "            # Initialize additional columns\n",
    "            source_system = \"\"\n",
    "            hana_cv_name = \"\"\n",
    "\n",
    "            # Check if object type is Data Source (case-insensitive) and technical name contains \"HANA\"\n",
    "            if \"data\" in object_type.lower() and \"source\" in object_type.lower() and \"hana\" in technical_name.lower():\n",
    "                # Collapse multiple spaces inside the technical name to a single space\n",
    "                technical_name = re.sub(r'\\s+', ' ', technical_name).strip()  # Collapse spaces\n",
    "                # Split the technical name by the first space\n",
    "                parts = technical_name.split(\" \", 1)\n",
    "                # Limit Technical Name to the first part (before the first space)\n",
    "                technical_name = parts[0]\n",
    "                # Extract Source System (everything after the first space)\n",
    "                if len(parts) > 1:\n",
    "                    source_system = parts[1].strip()  # Everything after the first part\n",
    "                # Extract Hana Calculation View Name from the transformed version of the 'Name'\n",
    "                transformed_name = re.sub(r'[/:->]', ' ', name)  # Replace the delimiters with spaces\n",
    "                hana_cv_name = transformed_name.split()[-1]  # Take the last part of the transformed string\n",
    "            # Append the row with the new columns\n",
    "            data.append([object_type, name, technical_name, source_system, hana_cv_name])\n",
    "            if cells[0].text.strip():\n",
    "                current_object_type = cells[0].text.strip()\n",
    "\n",
    "    # Create a pandas DataFrame with additional columns\n",
    "    df = pd.DataFrame(data, columns=[\"Object Type\", \"Name\", \"Technical Name\", \"Source System\", \"HANA CV Name\"])\n",
    "    return df\n",
    "\n",
    "def extract_transformation_names(df):\n",
    "    \"\"\"\n",
    "    Extracts values from the 'Technical Name' column for rows where 'Object Type' contains 'Transformation'.\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame.\n",
    "    Returns:\n",
    "    list: A list of extracted 'Technical Name' values.\n",
    "    \"\"\"\n",
    "    return df.loc[df['Object Type'].str.contains('Transformation', na=False, case=False), 'Technical Name'].tolist()\n",
    "\n",
    "def extract_tables_from_html(html, transformation_names):\n",
    "    \"\"\"\n",
    "    Extracts parent <table> elements containing <a name=\"TRFN{item}\"> from an HTML file.\n",
    "    Parameters:\n",
    "    html (str): The HTML file content.\n",
    "    transformation_names (list): List of transformation names.\n",
    "    Returns:\n",
    "    list: A list of extracted table HTML content as strings.\n",
    "    \"\"\"\n",
    "    extracted_tables = []\n",
    "    # Read the HTML file\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Iterate through transformation names\n",
    "    for name in transformation_names:\n",
    "        anchor_tag = soup.find(\"a\", {\"name\": f\"TRFN{name}\"})  # Find the first matching <a> tag\n",
    "        if anchor_tag:\n",
    "            table_tag = anchor_tag.find_parent(\"table\", {\n",
    "                \"width\": \"600\",\n",
    "                \"class\": \"SAPBEXBorderlessFlexBox\",\n",
    "                \"cellspacing\": \"0\",\n",
    "                \"cellpadding\": \"0\",\n",
    "                \"border\": \"1\"\n",
    "            })\n",
    "            if table_tag:\n",
    "                extracted_tables.append(str(table_tag))  # Store as HTML string\n",
    "    return extracted_tables\n",
    "\n",
    "def extract_transf_details(tables_list, file_names, output_dir):\n",
    "    \"\"\"\n",
    "    Extracts SAP transformation details from HTML tables using Azure OpenAI and saves the details to text files.\n",
    "    Parameters:\n",
    "    tables_list (list): List of HTML table strings.\n",
    "    file_names (list): List of filenames for storing transformation details.\n",
    "    output_dir (str): Directory where transformation details will be saved.\n",
    "    Returns:\n",
    "    list: List of model-generated transformation/table details.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    routines = []\n",
    "    for index, table_html in enumerate(tables_list):\n",
    "        extraction_prompt = f\"\"\"You are given an HTML structure below describing a SAP transformation.\n",
    "        Focus on the Key Rules and/or the Data Rules. Extract the required fields in csv format with\n",
    "        the following header: Source Name, Target Name, Source Field Name, Source Field Description,\n",
    "        Source Field Data Type, Source Field Length, Rule Type, Target InfoObject Name, Target InfoObject Description,\n",
    "        Target InfoObject Data Type, Target InfoObject Length. \\n\\nHTML structure:\\n{table_html}\"\"\"\n",
    "        abap_prompt = f\"\"\"You are given an HTML structure below describing a SAP transformation.\n",
    "        Focus on the ABAP code present in the structure. Extract the required code and store in JSON format\n",
    "        with the following keys: Start Routine, Global Code, Global Code 2, End Routine, Invers Endroutine.\n",
    "        Ignore any statements that say '... \"insert your code here', do not include them in the extract.\n",
    "        If you cannot find code for one key simply leave it blank. \\n\\nHTML structure:\\n{table_html}\"\"\"\n",
    "        try:\n",
    "            # Call Azure OpenAI model for chat completion\n",
    "            details_response = call_model(selected_model, prompt=extraction_prompt)\n",
    "            #details_response = call_model_o1(deployment_name=\"o1\", prompt=extraction_prompt)\n",
    "            summaries.append(details_response)\n",
    "            abap_response = call_model(selected_model, prompt=abap_prompt)\n",
    "            #abap_response = call_model_o1(deployment_name=\"o1\", prompt=abap_prompt)\n",
    "            routines.append(abap_response)\n",
    "            # Write the summary to a text file\n",
    "            file_name = f\"{file_names[index]}.txt\"\n",
    "            file_path = f\"{output_dir}/{file_name}\"\n",
    "            dbutils.fs.put(file_path, f\"Transformation details:\\n{details_response}\\n\\nTransformation ABAP code:\\n{abap_response}\\n\", overwrite=True)\n",
    "        except Exception as e:\n",
    "            log_message = f\"Error processing table {index}: {e}\"\n",
    "            dbutils.fs.put(f\"{output_dir}/error_{file_names[index]}.log\", log_message, overwrite=True)\n",
    "            log_messages.append(f\"**Error**: {log_message}\\n\")\n",
    "            print(log_message)\n",
    "    return summaries, routines\n",
    "\n",
    "def convert_transf(tables_details, routine_codes, file_names, output_dir):\n",
    "    \"\"\"\n",
    "    Translate SAP transformation details from detail tables using Azure OpenAI and saves the code to text files.\n",
    "    Parameters:\n",
    "    tables_details (list): List of table transformation details.\n",
    "    routine_codes (list): List of transformation ABAP code.\n",
    "    file_names (list): List of filenames for storing transformation details.\n",
    "    output_dir (str): Directory where transformation details will be saved.\n",
    "    Returns:\n",
    "    list: List of model-generated code that replicates transformation behaviour on Databricks.\n",
    "    \"\"\"\n",
    "    conversions = []\n",
    "    for index, table_html in enumerate(tables_details):\n",
    "        conversion_prompt = f\"\"\"Below you are given two major pieces of a SAP transformation, the transformation\n",
    "        details with source and target fields and the routine code used in the transformation. Using these details\n",
    "        do the following:\n",
    "        1. Write SQL code that runs on Databricks that builds the source table structure.\n",
    "        2. Write SQL code that runs on Databricks that builds the target table structure.\n",
    "        3. Write SQL code that runs on Databricks that writes data from source table to target table in the following order:\n",
    "            i. 1:1 mappings (i.e. \"[DIRECT]\")\n",
    "            ii. \"[CONSTANT]\" fields\n",
    "            iii. Transformation rules/logic in the routine code\n",
    "            iv. Any other logic deduced elsewhere (mention it explicitly in the output)\n",
    "        Optimize the code for Databricks. Assume that the environment in Databricks has already been created,\n",
    "        thus adhere strictly to generating the code for the two tables without any setup steps. Write comments\n",
    "        explaining code logic. \\n\\nTRANSFORMATION DETAILS:\\n{table_html} \\n\\nROUTINE CODE:\\n{routine_codes[index]}\"\"\"\n",
    "        try:\n",
    "            # Call Azure OpenAI model for chat completion\n",
    "            #response = call_model_o1(deployment_name=\"o1\", prompt=conversion_prompt)\n",
    "            response = call_model(selected_model, prompt=conversion_prompt)\n",
    "            conversions.append(response)\n",
    "            # Write the summary to a text file\n",
    "            file_name = f\"{file_names[index]}.txt\"\n",
    "            file_path = f\"{output_dir}/{file_name}\"\n",
    "            dbutils.fs.put(file_path, f\"Transformation code:\\n{response}\", overwrite=True)\n",
    "        except Exception as e:\n",
    "            log_message = f\"Error processing table {index}: {e}\"\n",
    "            dbutils.fs.put(f\"{output_dir}/error_{file_names[index]}.log\", log_message, overwrite=True)\n",
    "            log_messages.append(f\"**Error**: {log_message}\\n\")\n",
    "            print(log_message)\n",
    "    return conversions\n",
    "\n",
    "# Process the DataFlow\n",
    "print(f\"Processing DataFlow: {dataflow}\")\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "validated_directory_path = f\"{validated_directory_base_path}{dataflow}/\"\n",
    "archive_date_folder = f\"{archived_directory_base_path}{dataflow}/{current_date}/\"\n",
    "logs_directory_path = f\"{logs_directory_base_path}{dataflow}/{current_date}/\"\n",
    "input_directory_path = f\"{input_directory_base_path}{dataflow}/\"\n",
    "\n",
    "# Ensure output directories for the current dataflow exist\n",
    "dbutils.fs.mkdirs(validated_directory_path)\n",
    "dbutils.fs.mkdirs(archive_date_folder)\n",
    "dbutils.fs.mkdirs(logs_directory_path)\n",
    "\n",
    "# Get file info from config table (DataFlowBaseInfo)\n",
    "get_dataflowbaseinfo = f\"\"\"\n",
    "    SELECT * FROM codeconverter_config.dataflowbaseinfo\n",
    "    WHERE DataFlow = '{dataflow}'\n",
    "\"\"\"\n",
    "# Execute the query and assign to a DataFrame\n",
    "result_flowbaseinfo = spark.sql(get_dataflowbaseinfo)\n",
    "# Get values from DataFlowBaseInfo table\n",
    "dataflow_files_sourcepath_value = result_flowbaseinfo.select('DataFlowFilesSourceFolder').first()[0]\n",
    "\n",
    "# Set the source directory path based on the selected folder\n",
    "source_directory_path = f\"{base_url}{dataflow_files_sourcepath_value}/\"\n",
    "\n",
    "# List files in the source directory for the current dataflow\n",
    "files = dbutils.fs.ls(input_directory_path)\n",
    "\n",
    "# Retrieve the BW-related files for the current DataFlow\n",
    "bw_files_query = f\"SELECT SAPFileName FROM codeconverter_config.dataflowtableinfo WHERE SourceSystem = 'BW' AND DataFlow = '{dataflow}'\"\n",
    "bw_files_df = spark.sql(bw_files_query)\n",
    "\n",
    "# Convert the result into a list of file names\n",
    "bw_files = [row['SAPFileName'] for row in bw_files_df.collect()]\n",
    "\n",
    "# Keep only BW source files for processing\n",
    "bw_files_to_process = [file for file in files if file.name in bw_files]\n",
    "\n",
    "# Initialize a list to store all log messages for the current DataFlow\n",
    "log_messages = []\n",
    "\n",
    "# Process each file in the source directory\n",
    "for file_info in bw_files_to_process:\n",
    "    file_path = file_info.path\n",
    "    processed_html = process_html_spark(file_path)\n",
    "    extracted_table = extract_first_table(processed_html)\n",
    "    df = table_to_dataframe(extracted_table)\n",
    "    print(f'extracted_table = \\n{display(df)}')\n",
    "    transformations_technical_names = extract_transformation_names(df)\n",
    "    print(f'transformations_technical_names = {transformations_technical_names}')\n",
    "    transformation_tables = extract_tables_from_html(processed_html, transformations_technical_names)\n",
    "    transformation_details, transformation_routines = extract_transf_details(transformation_tables, transformations_technical_names, logs_directory_path)\n",
    "    print(f'transformation_details = {transformation_details}')\n",
    "    print(f'transformation_routines = {transformation_routines}')\n",
    "    transformation_code = convert_transf(transformation_details, transformation_routines, transformations_technical_names, validated_directory_path)\n",
    "    print(f'transformation_code = {transformation_code}')\n",
    "\n",
    "# Archive the original BW source files\n",
    "for file_info in bw_files_to_process:\n",
    "    file_name = file_info.name\n",
    "    file_path = file_info.path\n",
    "    try:\n",
    "        archive_file_path = f\"{archive_date_folder}{file_name}\"\n",
    "        dbutils.fs.mv(file_path, archive_file_path)\n",
    "        log_message = f\"Successfully archived the original file {file_name} to: {archive_file_path}\"\n",
    "        dbutils.fs.put(f\"{logs_directory_path}archive_{file_name}.log\", log_message, overwrite=True)\n",
    "        print(log_message)\n",
    "    except Exception as e:\n",
    "        log_message = f\"Error archiving file {file_name}: {e}\"\n",
    "        dbutils.fs.put(f\"{logs_directory_path}error_archive_{file_name}.log\", log_message, overwrite=True)\n",
    "        log_messages.append(f\"**Error**: {log_message}\\n\")\n",
    "        print(log_message)\n",
    "\n",
    "# Write all accumulated log messages to a final log file\n",
    "current_time = datetime.now().strftime('%H-%M-%S')\n",
    "final_log_filename = f\"final_log_{current_time}.log\"\n",
    "final_log_file_path = f\"{logs_directory_path}{final_log_filename}\"\n",
    "try:\n",
    "    dbutils.fs.put(final_log_file_path, \"\\n\".join(log_messages), overwrite=True)\n",
    "    print(f\"Final log written to: {final_log_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing final log: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CodeConvertFromHTMLtoSQL",
   "widgets": {
    "DataFlow": {
     "currentValue": "DF34",
     "nuid": "f80c81e7-a203-4f42-8c6b-13e2abb852d4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "DataFlow",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "DataFlow",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "model_choice": {
     "currentValue": "o1-preview",
     "nuid": "6b3f1385-3f91-4351-825a-a3fa142c9e07",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "o1",
      "label": "Choose model type",
      "name": "model_choice",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "o1",
        "o1-mini",
        "o1-preview"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "o1",
      "label": "Choose model type",
      "name": "model_choice",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "o1",
        "o1-mini",
        "o1-preview"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
