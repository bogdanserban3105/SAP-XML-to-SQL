{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd5002f-4a5c-449b-b2f0-1de4ad2941a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781ac1dc-be41-4bec-a39a-263dd1133702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Set your storage account SAS token and name\n",
    "sasToken = \"placeholder\"  # Replace with your actual SAS token\n",
    "sa = \"erdccalearning\"  # Replace with your actual storage account name\n",
    "container_name = \"nndemo\"  # Replace with your actual container name\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{sa}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{sa}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.nndemo.{sa}.blob.core.windows.net\", f\"{sasToken}\")\n",
    "\n",
    "# Set the base URL for the container and directories\n",
    "base_url = f\"wasbs://{container_name}@{sa}.blob.core.windows.net/AcceleratorSAPFiles/\"\n",
    "\n",
    "source_directory_path = f\"{base_url}ValidatedFiles/\"\n",
    "cleaned_directory_path = f\"{base_url}CleanedSQLFiles/\"\n",
    "archive_directory_path = f\"{base_url}ArchivedValidatedFiles/\"\n",
    "\n",
    "# Create an archive folder with the current date\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "archive_date_folder = f\"{archive_directory_path.rstrip('/')}/{current_date}/\"\n",
    "dbutils.fs.mkdirs(archive_date_folder)\n",
    "\n",
    "# List files in the source directory\n",
    "files = dbutils.fs.ls(source_directory_path)\n",
    "\n",
    "\n",
    "# This function performs a double cleaning operation on the model's output to ensure that only valid and well-formatted SQL queries are retained.\n",
    "\n",
    "def cleanse_sql_output(output: str) -> str:\n",
    "\n",
    "    # Remove the '```sql' markers (both the opening and closing markers)\n",
    "    cleaned_output = re.sub(r'```sql', '', output)\n",
    "    \n",
    "    # Remove all groups of backticks (`` ` `` or ````` or `````` etc.)\n",
    "    cleaned_output = re.sub(r'`+', '', cleaned_output)\n",
    "    \n",
    "    # Return the cleaned output\n",
    "    return cleaned_output\n",
    "\n",
    "\n",
    "def read_file_content(file_path):\n",
    "    \"\"\"Read the content of a file using Spark's higher-level API (spark.read.text).\"\"\"\n",
    "    try:\n",
    "        # Read the file content using spark.read.text\n",
    "        df = spark.read.text(file_path)\n",
    "        file_content = df.rdd.map(lambda r: r[0]).collect()\n",
    "        return file_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {file_path}\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process each file in the source directory\n",
    "for file_info in files:\n",
    "    file_path = file_info.path\n",
    "\n",
    "    if file_info.isFile():\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Read file content\n",
    "            file_content = read_file_content(file_path)\n",
    "            if file_content is None:\n",
    "                continue\n",
    "\n",
    "            # Convert file content to a single string\n",
    "            file_content_as_string = \"\\n\".join(file_content)\n",
    "\n",
    "            # Clean the SQL code using the model\n",
    "            prompt = f\"Clean the following SQL code by removing comments and unwanted characters, making it ready to run automatically by the engine without altering the logic. Please clean the following query by removing all backticks (`) from the text and keep only the last version of the sql squery. :\\n{file_content_as_string}\"\n",
    "            model_output = call_model_o1(deployment_name=\"o1\", prompt=prompt)\n",
    "\n",
    "            # Reassess the cleaned SQL code\n",
    "            reassess_prompt = (f\"Please confirm that the following SQL code has been cleaned correctly, without altering its logic, and is ready for automatic execution by the engine. Please make sure that the following query has all backticks (`) removed from the text and has only the last version of the sql query. When you write the output make sure to keep only the query:\\n\\nOriginal SQL code:\\n{file_content_as_string}\\n\\n\"\n",
    "                               f\"Cleaned SQL code:\\n{model_output}\")\n",
    "            reassessed_output = call_model_o1(deployment_name=\"o1\", prompt=reassess_prompt)\n",
    "\n",
    "            # double cleaning operation\n",
    "            reassessed_output = cleanse_sql_output(reassessed_output)\n",
    "\n",
    "            # Define the output file path\n",
    "            output_file_path = file_path.replace(source_directory_path, cleaned_directory_path)\n",
    "            \n",
    "            # Save the reassessed output to the cleaned directory\n",
    "            dbutils.fs.put(output_file_path, reassessed_output, overwrite=True)\n",
    "            print(f\"Successfully processed and saved cleaned SQL code for file: {file_path}\")\n",
    "\n",
    "            # Archive the original validated file to the archive directory\n",
    "            archive_file_path = f\"{archive_date_folder}{os.path.basename(file_path)}\"\n",
    "            dbutils.fs.mv(file_path, archive_file_path)\n",
    "            print(f\"Successfully archived the original validated file to: {archive_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping directory: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clean_sql_queries_old",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
