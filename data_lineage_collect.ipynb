{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cbfc46-5136-423f-9a61-5cd4a3d17a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. first the convert has to happen so that the sql is analyzed\n",
    "# 2. then the lineage is done. \n",
    "\n",
    "\n",
    "# to be situation: \n",
    "#     A process so that i can pick a flow and generate:\n",
    "#         1. lineage diagram starting from a CV\n",
    "#         2. Lineage for all the columns in the CV down to tables. \n",
    "#             table output should be: Nr of lines is = number of fields in the Top CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea5f4983-b6b6-47f6-a80d-dfd0220d814f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82751273-f364-4cde-b6d6-92829a687268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83486ac4-22f0-4f6d-9094-6a05bc12d3b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3494d4de-ca03-4452-89b2-e1a320f2c598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"data_flow\", \"\", \"Enter Data Flow Value: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8ead5d-89ca-49d7-ad85-4d4be956013e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sa = \"stlpdel01dev\"\n",
    "container_name = \"codeconverter\" # Replace with your actual container name\n",
    "spark.conf.set(\"fs.azure.account.auth.type\", \"CustomAccessToken\")\n",
    "spark.conf.set(\"fs.azure.account.custom.token.provider.class\", spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b772f87-545e-41db-bb6d-17e9e241d7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_flow = dbutils.widgets.get(\"data_flow\")\n",
    "base_url = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/Lineage_SAP/input/{data_flow}\"\n",
    "output_url = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/Lineage_SAP/output/{data_flow}/\"\n",
    "dbutils.fs.ls(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9faf25-28be-4216-b470-f7e87a023546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# List files in the specified directory\n",
    "files = dbutils.fs.ls(base_url)\n",
    "\n",
    "# Check if the directory is empty and print the output URL or a message\n",
    "if files:\n",
    "    print(files)\n",
    "else:\n",
    "    print(\"There is no file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c443365-8a65-4f37-b0e1-e63d85b9ddd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create the INSERT for source to target table/column/transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a746fb-4efe-4475-8127-84b3a25bb10f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "archive_directory_path = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/Lineage_SAP/InputArchivedFiles/\"\n",
    "print(archive_directory_path)\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "archive_input_date_folder = f\"{archive_directory_path.rstrip('/')}/{current_date}/\"\n",
    "dbutils.fs.mkdirs(archive_input_date_folder)\n",
    "\n",
    "files = dbutils.fs.ls(base_url)\n",
    "\n",
    "# Define the pattern to match everything ending with 'validated_code'\n",
    "pattern = r\".validated_code.*$\"\n",
    "\n",
    "\n",
    "catalog_schema = \"hive_metastore.codeconverter_config\"\n",
    "display(files)\n",
    "\n",
    "initial_prompt = f\"\"\"generate insert scripts for each column to to understand the source table. the target table will be the one from create statement. generate insert for a table having the columns: target table name, target column name, source table name, source column name, transformation. Consider that the lineage table name is {catalog_schema}.{data_flow.lower()}_lineage. \n",
    "In case a query is having multiple ctes, to take the source table go through all ctes and take the base table for that specific column.\n",
    "For each specific column go through all ctes and add all steps of transformations\n",
    "Generate only the insert statement without any other comments. In case there are 2 or more tables inserted into source_table_name column, give them aliases. Use these aliases in the source_column_name as well to know where the column is coming from. Generate multiple-row INSERT statement. \n",
    "All the inserted values should be string. Don't put column names in quotation marks\n",
    "Take into consideration all the above remarks above the table naming conventions.  \"\"\"\n",
    "\n",
    "for file_info in files:\n",
    "\n",
    "    file_content = read_file_content(file_info.path)\n",
    "    if file_content is None:\n",
    "                continue\n",
    "    # Convert file content to a single string\n",
    "    file_content_as_string = \"\\n\".join(file_content)\n",
    "    final_prompt = initial_prompt + file_content_as_string\n",
    "    model_output = call_model_o1(deployment_name=\"o1\", prompt=final_prompt)\n",
    "\n",
    "    #Ensure file_name is a string\n",
    "    file_name = str(file_info.name)\n",
    "    modified_file_name = re.sub(pattern, \"_lineage.txt\", file_name)\n",
    "    # Replace the matched pattern with '_lineage.txt'\n",
    "    output_file_path = output_url + modified_file_name\n",
    "\n",
    "    print(output_file_path)\n",
    "    \n",
    "    dbutils.fs.put(output_file_path, model_output, overwrite=True)\n",
    "\n",
    "    dbutils.fs.mv(file_info.path, archive_input_date_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68bfce4-b33f-4470-9d83-0e9d302f1df6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE hive_metastore.codeconverter_config.{data_flow.lower()}_lineage\n",
    "(\n",
    "  TARGET_TABLE_NAME string,\n",
    "TARGET_COLUMN_NAME string,\n",
    "SOURCE_TABLE_NAME string,\n",
    "SOURCE_COLUMN_NAME string,\n",
    "TRANSFORMATION string\n",
    ")\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a6df49d-736f-4a20-8abf-3cef26faf187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an archive folder with the current date\n",
    "archive_directory_path = f\"abfss://{container_name}@{sa}.dfs.core.windows.net/AcceleratorSAPFiles/Lineage_SAP/OutputArchivedFiles/\"\n",
    "print(archive_directory_path)\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "archive_date_folder = f\"{archive_directory_path.rstrip('/')}/{current_date}/\"\n",
    "dbutils.fs.mkdirs(archive_date_folder)\n",
    "\n",
    "files = dbutils.fs.ls(output_file_path)\n",
    "\n",
    "# Process each file in the cleaned directory\n",
    "for file_info in files:\n",
    "    file_path = file_info.path\n",
    "    if file_info.isFile():\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        try:\n",
    "            # Read file content\n",
    "            file_content = read_file_content(file_path)\n",
    "            if file_content is None:\n",
    "                continue\n",
    "            \n",
    "            # Convert file content to a single string\n",
    "            sql_code = \"\\n\".join(file_content)\n",
    "            \n",
    "            # Split the SQL code into individual statements\n",
    "            statements = sql_code.split(\";\")\n",
    "            \n",
    "            for statement in statements:\n",
    "                statement = statement.strip()\n",
    "                if statement:\n",
    "                    # Execute each SQL statement individually\n",
    "                    print(f\"Executing SQL statement: {statement}\")\n",
    "                    #spark.sql(statement)\n",
    "            \n",
    "            print(f\"Successfully executed SQL code for file: {file_path}\")\n",
    "            dbutils.fs.mv(file_path, archive_date_folder)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping directory: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f93502-28dd-417e-aa6d-0f35e3ed5400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_lineage = spark.sql(f\"\"\"SELECT * FROM codeconverter_config.{data_flow.lower()}_lineage\"\"\")\n",
    "display(df_lineage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "711f44c8-2e0b-4f22-855c-12281edc871c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#One row per column lineage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b15d84e-f8e3-442c-9ad0-84b7b75e6929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# # Query initial dataframe\n",
    "start_df = spark.table(f\"hive_metastore.codeconverter_config.{data_flow.lower()}_lineage\")\n",
    "\n",
    "# Retrieve list of unique target table names and target columns names\n",
    "# unique_filter = start_df.filter((col(\"target_table_name\").isin(\"nntst.CV_030_POLICY_DERIVATIONS\", \"nntst.CV_020_ZORRO_POL_TARIFF\"))\n",
    "#                                                & (col(\"target_column_name\").isin(\"ZSTDTPROJ\", \"ZSNP_DT_ULTIMO\")))\n",
    "# unique_tgt_tbl_col = unique_filter.select(\"target_column_name\", \"target_table_name\").distinct()\n",
    "\n",
    "# unique_filter = start_df.filter((col(\"target_table_name\").isin(\"nntst.CV_030_POLICY_DERIVATIONS\"))\n",
    "#                                                & (col(\"target_column_name\").isin(\"ZSTDTPROJ\")))\n",
    "unique_tgt_tbl_col = start_df.select(\"target_column_name\", \"target_table_name\").distinct()\n",
    "\n",
    "#display(unique_tgt_tbl_col)\n",
    "\n",
    "#Define an empty dataframe which will store the final lineage for all columns\n",
    "results_df = []\n",
    "\n",
    "# Iterate through all target columns and table names\n",
    "for row in unique_tgt_tbl_col.collect():\n",
    "    target_column_name = row[\"target_column_name\"]\n",
    "    target_table_name = row[\"target_table_name\"]\n",
    "\n",
    "    order_no = 1\n",
    "    lineage_df = start_df.filter((col(\"target_column_name\") == target_column_name) & \n",
    "                                 (col(\"target_table_name\") == target_table_name))\n",
    "    #lineage_df = lineage_df.withColumn(\"order_no\", lit(order_no))\n",
    "\n",
    "    current_item_df = lineage_df\n",
    "    while True:\n",
    "        next_item_df = current_item_df.alias(\"i\").join(start_df.alias(\"s\"), \n",
    "                                                       (col(\"i.source_table_name\") == col(\"s.target_table_name\")) & \n",
    "                                                       (col(\"i.source_column_name\") == col(\"s.target_column_name\")), \n",
    "                                                       how=\"inner\").select(\"s.*\")\n",
    "        order_no += 1\n",
    "        next_item_df = next_item_df.withColumn(\"order_no\", lit(order_no))\n",
    "\n",
    "        #renaming columns\n",
    "        next_item_df_renamed = next_item_df.withColumnRenamed(\"TARGET_TABLE_NAME\", f\"TARGET_TABLE_NAME{order_no}\")\n",
    "        next_item_df_renamed = next_item_df_renamed.withColumnRenamed(\"source_table_name\", f\"source_table_name{order_no}\")\n",
    "        next_item_df_renamed = next_item_df_renamed.withColumnRenamed(\"target_column_name\", f\"target_column_name{order_no}\")\n",
    "        next_item_df_renamed = next_item_df_renamed.withColumnRenamed(\"source_column_name\", f\"source_column_name{order_no}\")\n",
    "        next_item_df_renamed = next_item_df_renamed.withColumnRenamed(\"transformation\", f\"transformatin{order_no}\")\n",
    "        next_item_df_renamed = next_item_df_renamed.withColumnRenamed(\"order_no\", f\"order_no{order_no}\")\n",
    "\n",
    "        if next_item_df.count() == 0:\n",
    "            break\n",
    "        \n",
    "        lineage_df = lineage_df.crossJoin(next_item_df_renamed)\n",
    "        current_item_df = next_item_df\n",
    "        \n",
    "\n",
    "    # Add this lineage_df to the overall results_df list\n",
    "    results_df.append(lineage_df)\n",
    "\n",
    "# Union all results to get the final dataframe\n",
    "final_results_df = results_df[0]\n",
    "\n",
    "for df in results_df[1:]:\n",
    "    final_results_df = final_results_df.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "# Show final results (In actual usage, you might save this to a table)\n",
    "final_results_df.display()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b663a3e0-3aed-4ebf-b9db-f9dbb2fb009a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create lineage diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "86e170f1-fa03-422c-a44f-cdaa7b067469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# strict digraph {\n",
    "#     a [shape=\"ellipse\" style=\"filled\" fillcolor=\"#1f77b4\"]\n",
    "#     b [shape=\"polygon\" style=\"filled\" fillcolor=\"#ff7f0e\"]\n",
    "#     a -> b [fillcolor=\"#a6cee3\" color=\"#1f78b4\"]\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048c3dbe-6641-424a-b1b3-3b06440aa021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "\n",
    "lineage_df = spark.table(f\"codeconverter_config.{data_flow.lower()}_lineage\")\n",
    "\n",
    "lineage_distinct_df = lineage_df.select(\"target_table_name\", \"source_table_name\").distinct()\n",
    "lineage_col_trim_df = lineage_distinct_df.withColumn(\"target_table_name\", substring(\"target_table_name\", 7, 1000)).withColumn(\"source_table_name\", substring(\"source_table_name\", 7, 1000))\n",
    "\n",
    "#display(lineage_col_trim_df)\n",
    "\n",
    "table_list_target = [row[\"target_table_name\"] for row in lineage_col_trim_df.collect()]\n",
    "table_list_source = [row[\"source_table_name\"] for row in lineage_col_trim_df.collect()]\n",
    "table_list = [item for item in set(table_list_target + table_list_source) if item != \"\"]\n",
    "\n",
    "\n",
    "digraph_string = \"strict digraph {\"\n",
    "\n",
    "\n",
    "for table in table_list:\n",
    "    digraph_string = digraph_string + f'\\n \"{table}\" [shape=\"ellipse\" style=\"filled\" fillcolor=\"#1f77b4\"]'\n",
    "    #print(table)\n",
    "\n",
    "for row in lineage_col_trim_df.collect():\n",
    "    digraph_string += f'\\n \"{row[\"target_table_name\"]}\" -> \"{row[\"source_table_name\"]}\" [fillcolor=\"#a6cee3\" color=\"#1f78b4\"]'\n",
    "    #print(row[\"target_table_name\"],  row[\"source_table_name\"])\n",
    "\n",
    "digraph_string = digraph_string + \"\\n }\"\n",
    "\n",
    "print(digraph_string)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8416116081238902,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data_lineage_collect",
   "widgets": {
    "data_flow": {
     "currentValue": "DF34",
     "nuid": "602c6466-21d4-401b-9b09-483220161984",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Data Flow Value: ",
      "name": "data_flow",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Data Flow Value: ",
      "name": "data_flow",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
